use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tauri::{AppHandle, Emitter};
use tokio::io::{AsyncBufReadExt, AsyncWriteExt, BufReader};
use tokio::process::Command;
use tokio::sync::Mutex;

use serde::{Deserialize, Serialize};

use crate::db::models::{
    CreateTestResultInput, CreateArenaResultInput, CreateAbResultInput, CreateMatrixResultInput,
    CreateEvalResultInput, Persona, PersonaToolDefinition,
};
use super::types::EphemeralPersona;
use crate::db::repos::execution::test_runs as repo;
use crate::db::repos::lab::arena as arena_repo;
use crate::db::repos::lab::ab as ab_repo;
use crate::db::repos::lab::matrix as matrix_repo;
use crate::db::repos::lab::eval as eval_repo;
use crate::db::DbPool;

use super::eval::{self, EvalInput, WEIGHT_TOOL_ACCURACY, WEIGHT_OUTPUT_QUALITY, WEIGHT_PROTOCOL_COMPLIANCE};
use super::parser;
use super::prompt;
use super::types::*;

// ── Types ──────────────────────────────────────────────────────

/// Model configuration for a test run, passed from the frontend.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestModelConfig {
    pub id: String,
    pub provider: String,
    pub model: Option<String>,
    pub base_url: Option<String>,
    pub auth_token: Option<String>,
}

/// A test scenario generated by the coordinator LLM.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestScenario {
    pub name: String,
    pub description: String,
    pub input_data: Option<serde_json::Value>,
    pub mock_tools: Vec<MockToolResponse>,
    pub expected_behavior: String,
    pub expected_tool_sequence: Option<Vec<String>>,
    pub expected_protocols: Option<Vec<String>>,
}

/// A mock tool response within a scenario.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MockToolResponse {
    pub tool_name: String,
    pub description: Option<String>,
    pub mock_response: serde_json::Value,
}

/// Tauri event payload for test run progress.
#[derive(Debug, Clone, Serialize)]
pub struct TestRunStatusEvent {
    pub run_id: String,
    pub phase: String,
    pub scenarios_count: Option<usize>,
    pub current: Option<usize>,
    pub total: Option<usize>,
    pub model_id: Option<String>,
    pub scenario_name: Option<String>,
    pub status: Option<String>,
    pub scores: Option<TestScores>,
    pub summary: Option<serde_json::Value>,
    pub error: Option<String>,
    /// Emitted once during the "generated" phase so the frontend can save scenarios to a suite.
    pub scenarios: Option<Vec<TestScenario>>,
}

#[derive(Debug, Clone, Serialize)]
pub struct TestScores {
    pub tool_accuracy: Option<i32>,
    pub output_quality: Option<i32>,
    pub protocol_compliance: Option<i32>,
}

// ── Main entry point ───────────────────────────────────────────

/// Run a full test session: generate scenarios, execute across models, score, summarize.
/// If `preloaded_scenarios` is Some, skip generation and use those scenarios directly.
#[allow(clippy::too_many_arguments)]
pub async fn run_test(
    app: AppHandle,
    pool: DbPool,
    run_id: String,
    ephemeral: EphemeralPersona,
    model_configs: Vec<TestModelConfig>,
    _log_dir: PathBuf,
    cancelled: Arc<std::sync::atomic::AtomicBool>,
    use_case_filter: Option<String>,
    preloaded_scenarios: Option<Vec<TestScenario>>,
) {
    let persona = &ephemeral.persona;
    let tools = &ephemeral.tools;

    // Phase 1: Generate or load scenarios
    let scenarios = if let Some(preloaded) = preloaded_scenarios {
        if preloaded.is_empty() {
            finish_with_error(&app, &pool, &run_id, "Saved test suite has no scenarios");
            return;
        }
        preloaded
    } else {
        emit_status(&app, &run_id, "generating", None);

        match generate_scenarios(persona, tools, use_case_filter.as_deref()).await {
            Ok(s) if s.is_empty() => {
                finish_with_error(&app, &pool, &run_id, "No test scenarios were generated");
                return;
            }
            Ok(s) => s,
            Err(e) => {
                finish_with_error(&app, &pool, &run_id, &format!("Scenario generation failed: {e}"));
                return;
            }
        }
    };

    let scenario_count = scenarios.len();
    let _ = repo::update_run_status(
        &pool,
        &run_id,
        "running",
        Some(scenario_count as i32),
        None,
        None,
        None,
    );

    let _ = app.emit(
        "test-run-status",
        TestRunStatusEvent {
            run_id: run_id.clone(),
            phase: "generated".into(),
            scenarios_count: Some(scenario_count),
            current: None,
            total: None,
            model_id: None,
            scenario_name: None,
            status: None,
            scores: None,
            summary: None,
            error: None,
            scenarios: Some(scenarios.clone()),
        },
    );

    // Phase 2: Execute each scenario × model combination
    let total = scenario_count * model_configs.len();
    let mut current = 0usize;

    // Track results for summary
    #[allow(clippy::type_complexity)]
    let results_tracker: Arc<Mutex<Vec<(String, i32, i32, i32, f64, i64)>>> =
        Arc::new(Mutex::new(Vec::new()));

    for scenario in &scenarios {
        for model in &model_configs {
            // Check cancellation
            if cancelled.load(std::sync::atomic::Ordering::Acquire) {
                let _ = repo::update_run_status(
                    &pool, &run_id, "cancelled", None, None, None, None,
                );
                emit_status(&app, &run_id, "cancelled", None);
                return;
            }

            current += 1;

            let _ = app.emit(
                "test-run-status",
                TestRunStatusEvent {
                    run_id: run_id.clone(),
                    phase: "executing".into(),
                    scenarios_count: Some(scenario_count),
                    current: Some(current),
                    total: Some(total),
                    model_id: Some(model.id.clone()),
                    scenario_name: Some(scenario.name.clone()),
                    status: Some("running".into()),
                    scores: None,
                    summary: None,
                    error: None,
                    scenarios: None,
                },
            );

            let result = execute_scenario(persona, tools, scenario, model).await;

            let (status, scores) = match &result {
                Ok(r) => {
                    let s = score_result(r, scenario);
                    ("passed".to_string(), s)
                }
                Err(e) => (
                    "error".to_string(),
                    ScoreResult {
                        tool_accuracy: 0,
                        output_quality: 0,
                        protocol_compliance: 0,
                        output_preview: Some(e.clone()),
                        tool_calls_actual: None,
                        input_tokens: 0,
                        output_tokens: 0,
                        cost_usd: 0.0,
                        duration_ms: 0,
                        error_message: Some(e.clone()),
                    },
                ),
            };

            // Track for summary
            {
                let mut tracker = results_tracker.lock().await;
                tracker.push((
                    model.id.clone(),
                    scores.tool_accuracy,
                    scores.output_quality,
                    scores.protocol_compliance,
                    scores.cost_usd,
                    scores.duration_ms,
                ));
            }

            // Write result to DB
            let _ = repo::create_result(
                &pool,
                &CreateTestResultInput {
                    test_run_id: run_id.clone(),
                    scenario_name: scenario.name.clone(),
                    model_id: model.id.clone(),
                    provider: model.provider.clone(),
                    status: status.clone(),
                    output_preview: scores.output_preview.clone(),
                    tool_calls_expected: scenario
                        .expected_tool_sequence
                        .as_ref()
                        .map(|v| serde_json::to_string(v).unwrap_or_default()),
                    tool_calls_actual: scores.tool_calls_actual.clone(),
                    tool_accuracy_score: Some(scores.tool_accuracy),
                    output_quality_score: Some(scores.output_quality),
                    protocol_compliance: Some(scores.protocol_compliance),
                    input_tokens: scores.input_tokens,
                    output_tokens: scores.output_tokens,
                    cost_usd: scores.cost_usd,
                    duration_ms: scores.duration_ms,
                    error_message: scores.error_message.clone(),
                },
            );

            // Emit progress
            let _ = app.emit(
                "test-run-status",
                TestRunStatusEvent {
                    run_id: run_id.clone(),
                    phase: "executing".into(),
                    scenarios_count: Some(scenario_count),
                    current: Some(current),
                    total: Some(total),
                    model_id: Some(model.id.clone()),
                    scenario_name: Some(scenario.name.clone()),
                    status: Some(status),
                    scores: Some(TestScores {
                        tool_accuracy: Some(scores.tool_accuracy),
                        output_quality: Some(scores.output_quality),
                        protocol_compliance: Some(scores.protocol_compliance),
                    }),
                    summary: None,
                    error: scores.error_message,
                    scenarios: None,
                },
            );
        }
    }

    // Phase 3: Build summary
    let summary = build_summary(&results_tracker, &model_configs).await;
    let summary_str = serde_json::to_string(&summary).unwrap_or_default();
    let now = chrono::Utc::now().to_rfc3339();

    let _ = repo::update_run_status(
        &pool,
        &run_id,
        "completed",
        None,
        Some(&summary_str),
        None,
        Some(&now),
    );

    let _ = app.emit(
        "test-run-status",
        TestRunStatusEvent {
            run_id: run_id.clone(),
            phase: "completed".into(),
            scenarios_count: Some(scenario_count),
            current: Some(total),
            total: Some(total),
            model_id: None,
            scenario_name: None,
            status: None,
            scores: None,
            summary: Some(summary),
            error: None,
            scenarios: None,
        },
    );
}

// ── Phase 1: Generate scenarios ────────────────────────────────

async fn generate_scenarios(
    persona: &Persona,
    tools: &[PersonaToolDefinition],
    use_case_filter: Option<&str>,
) -> Result<Vec<TestScenario>, String> {
    let coordinator_prompt = build_coordinator_prompt(persona, tools, use_case_filter);

    let mut cli_args = prompt::build_cli_args(None, None);
    // Limit to 1 turn — we just want the JSON output
    cli_args.args.push("--max-turns".to_string());
    cli_args.args.push("1".to_string());

    let output = spawn_cli_and_collect(&cli_args, &coordinator_prompt).await?;

    // Extract JSON array from the output
    parse_scenarios_from_output(&output)
}

fn build_coordinator_prompt(persona: &Persona, tools: &[PersonaToolDefinition], use_case_filter: Option<&str>) -> String {
    let mut p = String::new();

    p.push_str("# Test Scenario Generator\n\n");
    p.push_str("You are a QA engineer generating test scenarios for an AI agent.\n\n");

    // Agent identity
    p.push_str("## Agent Under Test\n");
    p.push_str(&format!("**Name**: {}\n", persona.name));
    if let Some(ref desc) = persona.description {
        if !desc.is_empty() {
            p.push_str(&format!("**Description**: {}\n", desc));
        }
    }
    p.push('\n');

    // Agent prompt
    p.push_str("### Agent Prompt\n");
    if let Some(ref sp_json) = persona.structured_prompt {
        if let Ok(sp) = serde_json::from_str::<serde_json::Value>(sp_json) {
            for section in &["identity", "instructions", "toolGuidance", "examples", "errorHandling"] {
                if let Some(val) = sp.get(section).and_then(|v| v.as_str()) {
                    if !val.is_empty() {
                        p.push_str(&format!("**{}**: {}\n\n", section, val));
                    }
                }
            }
        }
    } else if !persona.system_prompt.is_empty() {
        p.push_str(&persona.system_prompt);
        p.push_str("\n\n");
    }

    // Available tools
    if !tools.is_empty() {
        p.push_str("### Available Tools\n");
        for tool in tools {
            p.push_str(&format!("- **{}** ({}): {}\n", tool.name, tool.category, tool.description));
            if let Some(ref schema) = tool.input_schema {
                p.push_str(&format!("  Input schema: {}\n", schema));
            }
        }
        p.push('\n');
    }

    // Task instructions
    p.push_str("## Your Task\n");
    p.push_str("Generate 3-5 realistic test scenarios for this agent. Each scenario must:\n");
    p.push_str("1. Represent a plausible real-world situation this agent would handle\n");
    p.push_str("2. Include realistic mock tool responses for every tool the agent might call\n");
    p.push_str("3. Describe the expected behavior and output\n\n");

    // Output format
    p.push_str("## Output Format\n");
    p.push_str("Respond with ONLY a JSON array (no markdown fences, no extra text):\n");
    p.push_str(r#"[{
  "name": "Short scenario name",
  "description": "What this scenario tests",
  "input_data": {},
  "mock_tools": [{
    "tool_name": "tool_name_here",
    "description": "What this mock simulates",
    "mock_response": {}
  }],
  "expected_behavior": "Description of what a good response looks like",
  "expected_tool_sequence": ["tool1", "tool2"],
  "expected_protocols": ["user_message"]
}]"#);

    // If a use case filter is provided, extract the matching use case from design_context
    // and append focused instructions
    if let Some(uc_id) = use_case_filter {
        if let Some(ref dc_json) = persona.design_context {
            if let Ok(dc) = serde_json::from_str::<serde_json::Value>(dc_json) {
                if let Some(use_cases) = dc.get("use_cases").and_then(|v| v.as_array()) {
                    for uc in use_cases {
                        let id = uc.get("id").and_then(|v| v.as_str()).unwrap_or("");
                        if id == uc_id {
                            let title = uc.get("title").and_then(|v| v.as_str()).unwrap_or("Unknown");
                            let desc = uc.get("description").and_then(|v| v.as_str()).unwrap_or("");
                            let category = uc.get("category").and_then(|v| v.as_str()).unwrap_or("");

                            p.push_str("\n\n## FOCUS: Specific Use Case\n");
                            p.push_str("Generate ALL test scenarios specifically for this use case:\n");
                            p.push_str(&format!("- **Title**: {}\n", title));
                            if !desc.is_empty() {
                                p.push_str(&format!("- **Description**: {}\n", desc));
                            }
                            if !category.is_empty() {
                                p.push_str(&format!("- **Category**: {}\n", category));
                            }

                            // Include sample_input if available
                            if let Some(sample) = uc.get("sample_input") {
                                if !sample.is_null() {
                                    p.push_str(&format!(
                                        "- **Sample Input**: {}\n",
                                        serde_json::to_string_pretty(sample).unwrap_or_default()
                                    ));
                                }
                            }

                            p.push_str("\nAll scenarios must be realistic variations of this specific use case. ");
                            p.push_str("Do NOT generate scenarios for other use cases.\n");
                            break;
                        }
                    }
                }
            }
        }
    }

    p
}

fn parse_scenarios_from_output(output: &str) -> Result<Vec<TestScenario>, String> {
    // Try to find a JSON array in the output
    // The output may contain other text before/after the JSON
    let trimmed = output.trim();

    // Try direct parse first
    if let Ok(scenarios) = serde_json::from_str::<Vec<TestScenario>>(trimmed) {
        return Ok(scenarios);
    }

    // Try to extract JSON array from the text
    if let Some(start) = trimmed.find('[') {
        if let Some(end) = trimmed.rfind(']') {
            let json_str = &trimmed[start..=end];
            if let Ok(scenarios) = serde_json::from_str::<Vec<TestScenario>>(json_str) {
                return Ok(scenarios);
            }
        }
    }

    Err(format!(
        "Failed to parse test scenarios from coordinator output. Raw output (first 500 chars): {}",
        &trimmed[..trimmed.len().min(500)]
    ))
}

// ── Phase 2: Execute scenario with a specific model ────────────

struct ScoreResult {
    tool_accuracy: i32,
    output_quality: i32,
    protocol_compliance: i32,
    output_preview: Option<String>,
    tool_calls_actual: Option<String>,
    input_tokens: i64,
    output_tokens: i64,
    cost_usd: f64,
    duration_ms: i64,
    error_message: Option<String>,
}

async fn execute_scenario(
    persona: &Persona,
    tools: &[PersonaToolDefinition],
    scenario: &TestScenario,
    model: &TestModelConfig,
) -> Result<ExecutionOutput, String> {
    // Build the base prompt
    let base_prompt = prompt::assemble_prompt(persona, tools, scenario.input_data.as_ref(), None, None);

    // Inject sandbox mock section before the EXECUTE NOW section
    let sandbox_section = build_sandbox_section(&scenario.mock_tools);
    let final_prompt = inject_sandbox_into_prompt(&base_prompt, &sandbox_section);

    // Build CLI args for this model
    let model_profile = ModelProfile {
        model: model.model.clone(),
        provider: Some(model.provider.clone()),
        base_url: model.base_url.clone(),
        auth_token: model.auth_token.clone(),
    };

    let mut cli_args = prompt::build_cli_args(None, Some(&model_profile));

    // Limit turns for sandbox testing
    cli_args.args.push("--max-turns".to_string());
    cli_args.args.push("3".to_string());

    // Run the CLI and collect structured output
    spawn_cli_and_collect_structured(&cli_args, &final_prompt).await
}

fn build_sandbox_section(mock_tools: &[MockToolResponse]) -> String {
    let mut section = String::new();
    section.push_str("\n## SANDBOX TESTING MODE — Simulated Tool Environment\n");
    section.push_str("You are running in test mode. Do NOT call actual tools.\n");
    section.push_str("Instead, use these simulated tool responses as if the tools returned them:\n\n");

    for mock in mock_tools {
        section.push_str(&format!("### Simulated response for `{}`\n", mock.tool_name));
        if let Some(ref desc) = mock.description {
            section.push_str(&format!("Context: {}\n", desc));
        }
        section.push_str("Assume it returns:\n```json\n");
        section.push_str(&serde_json::to_string_pretty(&mock.mock_response).unwrap_or_default());
        section.push_str("\n```\n\n");
    }

    section.push_str("Process these simulated results exactly as you would real tool responses.\n");
    section.push_str("Complete your full workflow and emit all appropriate protocol messages.\n");
    section.push_str("Do NOT mention that you are in test mode.\n\n");

    section
}

fn inject_sandbox_into_prompt(base_prompt: &str, sandbox_section: &str) -> String {
    // Insert the sandbox section before "## EXECUTE NOW" if it exists
    if let Some(pos) = base_prompt.find("## EXECUTE NOW") {
        let mut result = String::with_capacity(base_prompt.len() + sandbox_section.len());
        result.push_str(&base_prompt[..pos]);
        result.push_str(sandbox_section);
        result.push_str(&base_prompt[pos..]);
        result
    } else {
        // Fallback: append at end
        format!("{}\n{}", base_prompt, sandbox_section)
    }
}

// ── Scoring (delegates to unified eval framework) ──────────────

fn score_result(output: &ExecutionOutput, scenario: &TestScenario) -> ScoreResult {
    let expected_tools = scenario.expected_tool_sequence.as_deref();
    let expected_protocols = scenario.expected_protocols.as_deref();

    let eval_input = EvalInput {
        output: &output.assistant_text,
        expected_behavior: Some(&scenario.expected_behavior),
        expected_tools,
        actual_tools: Some(&output.tool_calls),
        expected_protocols,
        has_tools: true,
    };

    let keyword_result = eval::eval_keyword_match(&eval_input);
    let tool_result = eval::eval_tool_accuracy(&eval_input);
    let protocol_result = eval::eval_protocol_compliance(&eval_input);

    let tool_calls_json = if output.tool_calls.is_empty() {
        None
    } else {
        Some(serde_json::to_string(&output.tool_calls).unwrap_or_default())
    };

    let preview = if output.assistant_text.len() > 2000 {
        Some(output.assistant_text[..2000].to_string())
    } else if output.assistant_text.is_empty() {
        None
    } else {
        Some(output.assistant_text.clone())
    };

    ScoreResult {
        tool_accuracy: tool_result.score,
        output_quality: keyword_result.score,
        protocol_compliance: protocol_result.score,
        output_preview: preview,
        tool_calls_actual: tool_calls_json,
        input_tokens: output.input_tokens as i64,
        output_tokens: output.output_tokens as i64,
        cost_usd: output.cost_usd,
        duration_ms: output.duration_ms as i64,
        error_message: output.error.clone(),
    }
}

// ── Summary builder ────────────────────────────────────────────

#[allow(clippy::type_complexity)]
async fn build_summary(
    results: &Arc<Mutex<Vec<(String, i32, i32, i32, f64, i64)>>>,
    models: &[TestModelConfig],
) -> serde_json::Value {
    let data = results.lock().await;

    let mut per_model: HashMap<String, Vec<(i32, i32, i32, f64, i64)>> = HashMap::new();
    for (model_id, ta, oq, pc, cost, duration) in data.iter() {
        per_model
            .entry(model_id.clone())
            .or_default()
            .push((*ta, *oq, *pc, *cost, *duration));
    }

    let mut rankings: Vec<serde_json::Value> = Vec::new();

    for model in models {
        if let Some(results) = per_model.get(&model.id) {
            let count = results.len() as f64;
            let avg_ta = results.iter().map(|r| r.0 as f64).sum::<f64>() / count;
            let avg_oq = results.iter().map(|r| r.1 as f64).sum::<f64>() / count;
            let avg_pc = results.iter().map(|r| r.2 as f64).sum::<f64>() / count;
            let total_cost: f64 = results.iter().map(|r| r.3).sum();
            let avg_duration = results.iter().map(|r| r.4 as f64).sum::<f64>() / count;

            let composite = avg_ta * WEIGHT_TOOL_ACCURACY
                + avg_oq * WEIGHT_OUTPUT_QUALITY
                + avg_pc * WEIGHT_PROTOCOL_COMPLIANCE;
            let value_score = if total_cost > 0.0 {
                composite / (total_cost * 1000.0 + 1.0) * 100.0
            } else {
                composite // Free models get composite as value
            };

            rankings.push(serde_json::json!({
                "model_id": model.id,
                "provider": model.provider,
                "avg_tool_accuracy": avg_ta.round() as i32,
                "avg_output_quality": avg_oq.round() as i32,
                "avg_protocol_compliance": avg_pc.round() as i32,
                "composite_score": composite.round() as i32,
                "total_cost_usd": (total_cost * 10000.0).round() / 10000.0,
                "avg_duration_ms": avg_duration.round() as i64,
                "value_score": value_score.round() as i32,
                "scenarios_tested": count as i32,
            }));
        }
    }

    // Sort by composite score descending
    rankings.sort_by(|a, b| {
        let sa = a.get("composite_score").and_then(|v| v.as_i64()).unwrap_or(0);
        let sb = b.get("composite_score").and_then(|v| v.as_i64()).unwrap_or(0);
        sb.cmp(&sa)
    });

    let best_model = rankings
        .first()
        .and_then(|r| r.get("model_id"))
        .and_then(|v| v.as_str())
        .unwrap_or("unknown")
        .to_string();

    let best_value = rankings
        .iter()
        .max_by_key(|r| r.get("value_score").and_then(|v| v.as_i64()).unwrap_or(0))
        .and_then(|r| r.get("model_id"))
        .and_then(|v| v.as_str())
        .unwrap_or("unknown")
        .to_string();

    serde_json::json!({
        "best_quality_model": best_model,
        "best_value_model": best_value,
        "rankings": rankings,
    })
}

// ── CLI helpers ────────────────────────────────────────────────

/// Structured output from a CLI execution.
struct ExecutionOutput {
    assistant_text: String,
    tool_calls: Vec<String>,
    input_tokens: u64,
    output_tokens: u64,
    cost_usd: f64,
    duration_ms: u64,
    error: Option<String>,
}

/// Build a configured CLI `Command` with a temporary working directory.
///
/// Creates the temp dir, configures args, piped stdin/stdout, null stderr,
/// Windows `CREATE_NO_WINDOW` flag, and env overrides/removals.
/// Returns `(Command, exec_dir)` — the caller spawns the command, writes
/// to stdin, and collects output as needed.
pub(crate) fn build_cli_command(
    cli_args: &CliArgs,
    temp_dir_prefix: &str,
) -> Result<(Command, PathBuf), String> {
    let exec_dir = std::env::temp_dir().join(format!(
        "{}-{}",
        temp_dir_prefix,
        uuid::Uuid::new_v4()
    ));
    std::fs::create_dir_all(&exec_dir)
        .map_err(|e| format!("Failed to create temp dir: {e}"))?;

    let mut cmd = Command::new(&cli_args.command);
    cmd.args(&cli_args.args)
        .current_dir(&exec_dir)
        .stdin(std::process::Stdio::piped())
        .stdout(std::process::Stdio::piped())
        .stderr(std::process::Stdio::null());

    #[cfg(windows)]
    {
        #[allow(unused_imports)]
        use std::os::windows::process::CommandExt;
        cmd.creation_flags(0x08000000);
    }

    for key in &cli_args.env_removals {
        cmd.env_remove(key);
    }
    for (key, val) in &cli_args.env_overrides {
        cmd.env(key, val);
    }

    Ok((cmd, exec_dir))
}

/// Write the prompt to stdin and shut it down.
pub(crate) async fn write_prompt_to_stdin(child: &mut tokio::process::Child, prompt_text: &str) {
    if let Some(mut stdin) = child.stdin.take() {
        let _ = stdin.write_all(prompt_text.as_bytes()).await;
        let _ = stdin.shutdown().await;
    }
}

/// Spawn Claude CLI, pipe prompt to stdin, collect all output as a plain string.
/// Used for the coordinator (scenario generation).
async fn spawn_cli_and_collect(cli_args: &CliArgs, prompt_text: &str) -> Result<String, String> {
    let (mut cmd, exec_dir) = build_cli_command(cli_args, "personas-test-coord")?;
    let mut child = cmd.spawn().map_err(|e| format!("Failed to spawn CLI: {e}"))?;
    write_prompt_to_stdin(&mut child, prompt_text).await;

    let stdout = child.stdout.take().ok_or("No stdout")?;
    let mut reader = BufReader::new(stdout).lines();
    let mut assistant_text = String::new();

    let timeout = tokio::time::Duration::from_secs(120);
    let result = tokio::time::timeout(timeout, async {
        while let Ok(Some(line)) = reader.next_line().await {
            let (line_type, _) = parser::parse_stream_line(&line);
            if let StreamLineType::AssistantText { text } = line_type {
                assistant_text.push_str(&text);
                assistant_text.push('\n');
            }
        }
    })
    .await;

    let _ = child.wait().await;
    let _ = std::fs::remove_dir_all(&exec_dir);

    if result.is_err() {
        return Err("Coordinator CLI timed out after 120 seconds".into());
    }

    Ok(assistant_text)
}

/// Spawn Claude CLI, pipe prompt to stdin, collect structured execution output.
/// Used for the per-model persona execution.
async fn spawn_cli_and_collect_structured(
    cli_args: &CliArgs,
    prompt_text: &str,
) -> Result<ExecutionOutput, String> {
    let (mut cmd, exec_dir) = build_cli_command(cli_args, "personas-test-exec")?;
    let start = std::time::Instant::now();
    let mut child = cmd.spawn().map_err(|e| format!("Failed to spawn CLI: {e}"))?;
    write_prompt_to_stdin(&mut child, prompt_text).await;

    let stdout = child.stdout.take().ok_or("No stdout")?;
    let mut reader = BufReader::new(stdout).lines();

    let mut assistant_text = String::new();
    let mut tool_calls: Vec<String> = Vec::new();
    let mut metrics = ExecutionMetrics::default();

    let timeout = tokio::time::Duration::from_secs(180);
    let stream_result = tokio::time::timeout(timeout, async {
        while let Ok(Some(line)) = reader.next_line().await {
            let (line_type, _) = parser::parse_stream_line(&line);

            match line_type {
                StreamLineType::AssistantText { text } => {
                    assistant_text.push_str(&text);
                    assistant_text.push('\n');
                }
                StreamLineType::AssistantToolUse { tool_name, .. } => {
                    tool_calls.push(tool_name);
                }
                StreamLineType::Result { .. } => {
                    parser::update_metrics_from_result(&mut metrics, &line_type);
                }
                _ => {}
            }
        }
    })
    .await;

    let exit = child.wait().await;
    let duration_ms = start.elapsed().as_millis() as u64;
    let _ = std::fs::remove_dir_all(&exec_dir);

    let error = if stream_result.is_err() {
        Some("Execution timed out after 180 seconds".to_string())
    } else if let Ok(status) = exit {
        if !status.success() {
            Some(format!("CLI exited with code {}", status.code().unwrap_or(-1)))
        } else {
            None
        }
    } else {
        None
    };

    Ok(ExecutionOutput {
        assistant_text,
        tool_calls,
        input_tokens: metrics.input_tokens,
        output_tokens: metrics.output_tokens,
        cost_usd: metrics.cost_usd,
        duration_ms,
        error,
    })
}

// ── Utility helpers ────────────────────────────────────────────

fn emit_status(app: &AppHandle, run_id: &str, phase: &str, error: Option<&str>) {
    let _ = app.emit(
        "test-run-status",
        TestRunStatusEvent {
            run_id: run_id.to_string(),
            phase: phase.to_string(),
            scenarios_count: None,
            current: None,
            total: None,
            model_id: None,
            scenario_name: None,
            status: None,
            scores: None,
            summary: None,
            error: error.map(|s| s.to_string()),
            scenarios: None,
        },
    );
}

fn finish_with_error(app: &AppHandle, pool: &DbPool, run_id: &str, error: &str) {
    let now = chrono::Utc::now().to_rfc3339();
    let _ = repo::update_run_status(pool, run_id, "failed", None, None, Some(error), Some(&now));
    emit_status(app, run_id, "failed", Some(error));
}

// ============================================================================
// Lab: Arena — same flow as run_test but writes to lab_arena tables
// ============================================================================

fn emit_lab_status(app: &AppHandle, event_name: &str, run_id: &str, phase: &str, error: Option<&str>) {
    let _ = app.emit(
        event_name,
        TestRunStatusEvent {
            run_id: run_id.to_string(),
            phase: phase.to_string(),
            scenarios_count: None,
            current: None,
            total: None,
            model_id: None,
            scenario_name: None,
            status: None,
            scores: None,
            summary: None,
            error: error.map(|s| s.to_string()),
            scenarios: None,
        },
    );
}

#[allow(clippy::too_many_arguments, clippy::type_complexity)]
pub async fn run_arena_test(
    app: AppHandle,
    pool: DbPool,
    run_id: String,
    ephemeral: EphemeralPersona,
    model_configs: Vec<TestModelConfig>,
    _log_dir: std::path::PathBuf,
    cancelled: Arc<std::sync::atomic::AtomicBool>,
    use_case_filter: Option<String>,
) {
    let persona = &ephemeral.persona;
    let tools = &ephemeral.tools;
    emit_lab_status(&app, "lab-arena-status", &run_id, "generating", None);

    let scenarios = match generate_scenarios(persona, tools, use_case_filter.as_deref()).await {
        Ok(s) if s.is_empty() => {
            let now = chrono::Utc::now().to_rfc3339();
            let _ = arena_repo::update_run_status(&pool, &run_id, "failed", None, None, Some("No test scenarios were generated"), Some(&now));
            emit_lab_status(&app, "lab-arena-status", &run_id, "failed", Some("No test scenarios were generated"));
            return;
        }
        Ok(s) => s,
        Err(e) => {
            let msg = format!("Scenario generation failed: {e}");
            let now = chrono::Utc::now().to_rfc3339();
            let _ = arena_repo::update_run_status(&pool, &run_id, "failed", None, None, Some(&msg), Some(&now));
            emit_lab_status(&app, "lab-arena-status", &run_id, "failed", Some(&msg));
            return;
        }
    };

    let scenario_count = scenarios.len();
    let _ = arena_repo::update_run_status(&pool, &run_id, "running", Some(scenario_count as i32), None, None, None);

    let _ = app.emit("lab-arena-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "generated".into(),
        scenarios_count: Some(scenario_count), current: None, total: None,
        model_id: None, scenario_name: None, status: None, scores: None, summary: None, error: None,
        scenarios: None,
    });

    let total = scenario_count * model_configs.len();
    let mut current = 0usize;
    let results_tracker: Arc<Mutex<Vec<(String, i32, i32, i32, f64, i64)>>> = Arc::new(Mutex::new(Vec::new()));

    for scenario in &scenarios {
        for model in &model_configs {
            if cancelled.load(std::sync::atomic::Ordering::Acquire) {
                let _ = arena_repo::update_run_status(&pool, &run_id, "cancelled", None, None, None, None);
                emit_lab_status(&app, "lab-arena-status", &run_id, "cancelled", None);
                return;
            }

            current += 1;

            let _ = app.emit("lab-arena-status", TestRunStatusEvent {
                run_id: run_id.clone(), phase: "executing".into(),
                scenarios_count: Some(scenario_count), current: Some(current), total: Some(total),
                model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                status: Some("running".into()), scores: None, summary: None, error: None,
                scenarios: None,
            });

            let result = execute_scenario(persona, tools, scenario, model).await;

            let (status, scores) = match &result {
                Ok(r) => ("passed".to_string(), score_result(r, scenario)),
                Err(e) => ("error".to_string(), ScoreResult {
                    tool_accuracy: 0, output_quality: 0, protocol_compliance: 0,
                    output_preview: Some(e.clone()), tool_calls_actual: None,
                    input_tokens: 0, output_tokens: 0, cost_usd: 0.0, duration_ms: 0,
                    error_message: Some(e.clone()),
                }),
            };

            {
                let mut tracker = results_tracker.lock().await;
                tracker.push((model.id.clone(), scores.tool_accuracy, scores.output_quality, scores.protocol_compliance, scores.cost_usd, scores.duration_ms));
            }

            let _ = arena_repo::create_result(&pool, &CreateArenaResultInput {
                run_id: run_id.clone(),
                scenario_name: scenario.name.clone(),
                model_id: model.id.clone(),
                provider: model.provider.clone(),
                status: status.clone(),
                output_preview: scores.output_preview.clone(),
                tool_calls_expected: scenario.expected_tool_sequence.as_ref().map(|v| serde_json::to_string(v).unwrap_or_default()),
                tool_calls_actual: scores.tool_calls_actual.clone(),
                tool_accuracy_score: Some(scores.tool_accuracy),
                output_quality_score: Some(scores.output_quality),
                protocol_compliance: Some(scores.protocol_compliance),
                input_tokens: scores.input_tokens,
                output_tokens: scores.output_tokens,
                cost_usd: scores.cost_usd,
                duration_ms: scores.duration_ms,
                error_message: scores.error_message.clone(),
            });

            let _ = app.emit("lab-arena-status", TestRunStatusEvent {
                run_id: run_id.clone(), phase: "executing".into(),
                scenarios_count: Some(scenario_count), current: Some(current), total: Some(total),
                model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                status: Some(status),
                scores: Some(TestScores { tool_accuracy: Some(scores.tool_accuracy), output_quality: Some(scores.output_quality), protocol_compliance: Some(scores.protocol_compliance) }),
                summary: None, error: scores.error_message,
                scenarios: None,
            });
        }
    }

    let summary = build_summary(&results_tracker, &model_configs).await;
    let summary_str = serde_json::to_string(&summary).unwrap_or_default();
    let now = chrono::Utc::now().to_rfc3339();
    let _ = arena_repo::update_run_status(&pool, &run_id, "completed", None, Some(&summary_str), None, Some(&now));

    let _ = app.emit("lab-arena-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "completed".into(),
        scenarios_count: Some(scenario_count), current: Some(total), total: Some(total),
        model_id: None, scenario_name: None, status: None, scores: None,
        summary: Some(summary), error: None,
        scenarios: None,
    });
}

// ============================================================================
// Lab: A/B — runs scenarios across two prompt versions × models
// ============================================================================

#[allow(clippy::too_many_arguments, clippy::type_complexity)]
pub async fn run_ab_test(
    app: AppHandle,
    pool: DbPool,
    run_id: String,
    variants: Vec<(String, i32, Persona)>, // (version_id, version_number, persona_with_that_version)
    tools: Vec<PersonaToolDefinition>,
    model_configs: Vec<TestModelConfig>,
    cancelled: Arc<std::sync::atomic::AtomicBool>,
    use_case_filter: Option<String>,
) {
    emit_lab_status(&app, "lab-ab-status", &run_id, "generating", None);

    // Generate scenarios from the first variant (primary)
    let primary_persona = &variants[0].2;
    let scenarios = match generate_scenarios(primary_persona, &tools, use_case_filter.as_deref()).await {
        Ok(s) if s.is_empty() => {
            let now = chrono::Utc::now().to_rfc3339();
            let _ = ab_repo::update_run_status(&pool, &run_id, "failed", None, None, Some("No test scenarios were generated"), Some(&now));
            emit_lab_status(&app, "lab-ab-status", &run_id, "failed", Some("No test scenarios were generated"));
            return;
        }
        Ok(s) => s,
        Err(e) => {
            let msg = format!("Scenario generation failed: {e}");
            let now = chrono::Utc::now().to_rfc3339();
            let _ = ab_repo::update_run_status(&pool, &run_id, "failed", None, None, Some(&msg), Some(&now));
            emit_lab_status(&app, "lab-ab-status", &run_id, "failed", Some(&msg));
            return;
        }
    };

    let scenario_count = scenarios.len();
    let _ = ab_repo::update_run_status(&pool, &run_id, "running", Some(scenario_count as i32), None, None, None);

    let _ = app.emit("lab-ab-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "generated".into(),
        scenarios_count: Some(scenario_count), current: None, total: None,
        model_id: None, scenario_name: None, status: None, scores: None, summary: None, error: None,
        scenarios: None,
    });

    let total = scenario_count * model_configs.len() * variants.len();
    let mut current = 0usize;
    let results_tracker: Arc<Mutex<HashMap<String, Vec<(i32, i32, i32, f64, i64)>>>> = Arc::new(Mutex::new(HashMap::new()));

    for scenario in &scenarios {
        for model in &model_configs {
            for (version_id, version_num, persona_variant) in &variants {
                if cancelled.load(std::sync::atomic::Ordering::Acquire) {
                    let _ = ab_repo::update_run_status(&pool, &run_id, "cancelled", None, None, None, None);
                    emit_lab_status(&app, "lab-ab-status", &run_id, "cancelled", None);
                    return;
                }

                current += 1;

                let _ = app.emit("lab-ab-status", TestRunStatusEvent {
                    run_id: run_id.clone(), phase: "executing".into(),
                    scenarios_count: Some(scenario_count), current: Some(current), total: Some(total),
                    model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                    status: Some("running".into()), scores: None, summary: None, error: None,
                    scenarios: None,
                });

                let result = execute_scenario(persona_variant, &tools, scenario, model).await;

                let (status, scores) = match &result {
                    Ok(r) => ("passed".to_string(), score_result(r, scenario)),
                    Err(e) => ("error".to_string(), ScoreResult {
                        tool_accuracy: 0, output_quality: 0, protocol_compliance: 0,
                        output_preview: Some(e.clone()), tool_calls_actual: None,
                        input_tokens: 0, output_tokens: 0, cost_usd: 0.0, duration_ms: 0,
                        error_message: Some(e.clone()),
                    }),
                };

                {
                    let key = format!("v{}:{}", version_num, model.id);
                    let mut tracker = results_tracker.lock().await;
                    tracker.entry(key).or_default().push((
                        scores.tool_accuracy, scores.output_quality, scores.protocol_compliance,
                        scores.cost_usd, scores.duration_ms,
                    ));
                }

                let _ = ab_repo::create_result(&pool, &CreateAbResultInput {
                    run_id: run_id.clone(),
                    version_id: version_id.clone(),
                    version_number: *version_num,
                    scenario_name: scenario.name.clone(),
                    model_id: model.id.clone(),
                    provider: model.provider.clone(),
                    status: status.clone(),
                    output_preview: scores.output_preview.clone(),
                    tool_calls_expected: scenario.expected_tool_sequence.as_ref().map(|v| serde_json::to_string(v).unwrap_or_default()),
                    tool_calls_actual: scores.tool_calls_actual.clone(),
                    tool_accuracy_score: Some(scores.tool_accuracy),
                    output_quality_score: Some(scores.output_quality),
                    protocol_compliance: Some(scores.protocol_compliance),
                    input_tokens: scores.input_tokens,
                    output_tokens: scores.output_tokens,
                    cost_usd: scores.cost_usd,
                    duration_ms: scores.duration_ms,
                    error_message: scores.error_message.clone(),
                });

                let _ = app.emit("lab-ab-status", TestRunStatusEvent {
                    run_id: run_id.clone(), phase: "executing".into(),
                    scenarios_count: Some(scenario_count), current: Some(current), total: Some(total),
                    model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                    status: Some(status),
                    scores: Some(TestScores { tool_accuracy: Some(scores.tool_accuracy), output_quality: Some(scores.output_quality), protocol_compliance: Some(scores.protocol_compliance) }),
                    summary: None, error: scores.error_message,
                    scenarios: None,
                });
            }
        }
    }

    // Build A/B summary
    let tracker_data = results_tracker.lock().await;
    let mut summary_obj = serde_json::Map::new();
    for (key, results) in tracker_data.iter() {
        let count = results.len() as f64;
        let avg_ta = results.iter().map(|r| r.0 as f64).sum::<f64>() / count;
        let avg_oq = results.iter().map(|r| r.1 as f64).sum::<f64>() / count;
        let avg_pc = results.iter().map(|r| r.2 as f64).sum::<f64>() / count;
        let total_cost: f64 = results.iter().map(|r| r.3).sum();
        let composite = avg_ta * WEIGHT_TOOL_ACCURACY + avg_oq * WEIGHT_OUTPUT_QUALITY + avg_pc * WEIGHT_PROTOCOL_COMPLIANCE;
        summary_obj.insert(key.clone(), serde_json::json!({
            "avg_tool_accuracy": avg_ta.round() as i32,
            "avg_output_quality": avg_oq.round() as i32,
            "avg_protocol_compliance": avg_pc.round() as i32,
            "composite_score": composite.round() as i32,
            "total_cost_usd": (total_cost * 10000.0).round() / 10000.0,
            "scenarios_tested": count as i32,
        }));
    }
    drop(tracker_data);

    let summary = serde_json::Value::Object(summary_obj);
    let summary_str = serde_json::to_string(&summary).unwrap_or_default();
    let now = chrono::Utc::now().to_rfc3339();
    let _ = ab_repo::update_run_status(&pool, &run_id, "completed", None, Some(&summary_str), None, Some(&now));

    let _ = app.emit("lab-ab-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "completed".into(),
        scenarios_count: Some(scenario_count), current: Some(total), total: Some(total),
        model_id: None, scenario_name: None, status: None, scores: None,
        summary: Some(summary), error: None,
        scenarios: None,
    });
}

// ============================================================================
// Lab: Eval — N prompt versions × M models evaluation matrix
// ============================================================================

#[allow(clippy::too_many_arguments, clippy::type_complexity)]
pub async fn run_eval_test(
    app: AppHandle,
    pool: DbPool,
    run_id: String,
    variants: Vec<(String, i32, Persona)>, // (version_id, version_number, persona_with_that_version)
    tools: Vec<PersonaToolDefinition>,
    model_configs: Vec<TestModelConfig>,
    cancelled: Arc<std::sync::atomic::AtomicBool>,
    use_case_filter: Option<String>,
) {
    emit_lab_status(&app, "lab-eval-status", &run_id, "generating", None);

    // Generate scenarios from the first variant (primary)
    let primary_persona = &variants[0].2;
    let scenarios = match generate_scenarios(primary_persona, &tools, use_case_filter.as_deref()).await {
        Ok(s) if s.is_empty() => {
            let now = chrono::Utc::now().to_rfc3339();
            let _ = eval_repo::update_run_status(&pool, &run_id, "failed", None, None, Some("No test scenarios were generated"), Some(&now));
            emit_lab_status(&app, "lab-eval-status", &run_id, "failed", Some("No test scenarios were generated"));
            return;
        }
        Ok(s) => s,
        Err(e) => {
            let msg = format!("Scenario generation failed: {e}");
            let now = chrono::Utc::now().to_rfc3339();
            let _ = eval_repo::update_run_status(&pool, &run_id, "failed", None, None, Some(&msg), Some(&now));
            emit_lab_status(&app, "lab-eval-status", &run_id, "failed", Some(&msg));
            return;
        }
    };

    let scenario_count = scenarios.len();
    let _ = eval_repo::update_run_status(&pool, &run_id, "running", Some(scenario_count as i32), None, None, None);

    let _ = app.emit("lab-eval-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "generated".into(),
        scenarios_count: Some(scenario_count), current: None, total: None,
        model_id: None, scenario_name: None, status: None, scores: None, summary: None, error: None,
        scenarios: None,
    });

    let total = scenario_count * model_configs.len() * variants.len();
    let mut current = 0usize;
    let results_tracker: Arc<Mutex<HashMap<String, Vec<(i32, i32, i32, f64, i64)>>>> = Arc::new(Mutex::new(HashMap::new()));

    for scenario in &scenarios {
        for model in &model_configs {
            for (version_id, version_num, persona_variant) in &variants {
                if cancelled.load(std::sync::atomic::Ordering::Acquire) {
                    let _ = eval_repo::update_run_status(&pool, &run_id, "cancelled", None, None, None, None);
                    emit_lab_status(&app, "lab-eval-status", &run_id, "cancelled", None);
                    return;
                }

                current += 1;

                let _ = app.emit("lab-eval-status", TestRunStatusEvent {
                    run_id: run_id.clone(), phase: "executing".into(),
                    scenarios_count: Some(scenario_count), current: Some(current), total: Some(total),
                    model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                    status: Some("running".into()), scores: None, summary: None, error: None,
                    scenarios: None,
                });

                let result = execute_scenario(persona_variant, &tools, scenario, model).await;

                let (status, scores) = match &result {
                    Ok(r) => ("passed".to_string(), score_result(r, scenario)),
                    Err(e) => ("error".to_string(), ScoreResult {
                        tool_accuracy: 0, output_quality: 0, protocol_compliance: 0,
                        output_preview: Some(e.clone()), tool_calls_actual: None,
                        input_tokens: 0, output_tokens: 0, cost_usd: 0.0, duration_ms: 0,
                        error_message: Some(e.clone()),
                    }),
                };

                {
                    let key = format!("v{}:{}", version_num, model.id);
                    let mut tracker = results_tracker.lock().await;
                    tracker.entry(key).or_default().push((
                        scores.tool_accuracy, scores.output_quality, scores.protocol_compliance,
                        scores.cost_usd, scores.duration_ms,
                    ));
                }

                let _ = eval_repo::create_result(&pool, &CreateEvalResultInput {
                    run_id: run_id.clone(),
                    version_id: version_id.clone(),
                    version_number: *version_num,
                    scenario_name: scenario.name.clone(),
                    model_id: model.id.clone(),
                    provider: model.provider.clone(),
                    status: status.clone(),
                    output_preview: scores.output_preview.clone(),
                    tool_calls_expected: scenario.expected_tool_sequence.as_ref().map(|v| serde_json::to_string(v).unwrap_or_default()),
                    tool_calls_actual: scores.tool_calls_actual.clone(),
                    tool_accuracy_score: Some(scores.tool_accuracy),
                    output_quality_score: Some(scores.output_quality),
                    protocol_compliance: Some(scores.protocol_compliance),
                    input_tokens: scores.input_tokens,
                    output_tokens: scores.output_tokens,
                    cost_usd: scores.cost_usd,
                    duration_ms: scores.duration_ms,
                    error_message: scores.error_message.clone(),
                });

                let _ = app.emit("lab-eval-status", TestRunStatusEvent {
                    run_id: run_id.clone(), phase: "executing".into(),
                    scenarios_count: Some(scenario_count), current: Some(current), total: Some(total),
                    model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                    status: Some(status),
                    scores: Some(TestScores { tool_accuracy: Some(scores.tool_accuracy), output_quality: Some(scores.output_quality), protocol_compliance: Some(scores.protocol_compliance) }),
                    summary: None, error: scores.error_message,
                    scenarios: None,
                });
            }
        }
    }

    // Build eval summary
    let tracker_data = results_tracker.lock().await;
    let mut summary_obj = serde_json::Map::new();
    for (key, results) in tracker_data.iter() {
        let count = results.len() as f64;
        let avg_ta = results.iter().map(|r| r.0 as f64).sum::<f64>() / count;
        let avg_oq = results.iter().map(|r| r.1 as f64).sum::<f64>() / count;
        let avg_pc = results.iter().map(|r| r.2 as f64).sum::<f64>() / count;
        let total_cost: f64 = results.iter().map(|r| r.3).sum();
        let composite = avg_ta * WEIGHT_TOOL_ACCURACY + avg_oq * WEIGHT_OUTPUT_QUALITY + avg_pc * WEIGHT_PROTOCOL_COMPLIANCE;
        summary_obj.insert(key.clone(), serde_json::json!({
            "avg_tool_accuracy": avg_ta.round() as i32,
            "avg_output_quality": avg_oq.round() as i32,
            "avg_protocol_compliance": avg_pc.round() as i32,
            "composite_score": composite.round() as i32,
            "total_cost_usd": (total_cost * 10000.0).round() / 10000.0,
            "scenarios_tested": count as i32,
        }));
    }
    drop(tracker_data);

    let summary = serde_json::Value::Object(summary_obj);
    let summary_str = serde_json::to_string(&summary).unwrap_or_default();
    let now = chrono::Utc::now().to_rfc3339();
    let _ = eval_repo::update_run_status(&pool, &run_id, "completed", None, Some(&summary_str), None, Some(&now));

    let _ = app.emit("lab-eval-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "completed".into(),
        scenarios_count: Some(scenario_count), current: Some(total), total: Some(total),
        model_id: None, scenario_name: None, status: None, scores: None,
        summary: Some(summary), error: None,
        scenarios: None,
    });
}

// ============================================================================
// Lab: Matrix — draft generation + current vs draft comparison
// ============================================================================

#[allow(clippy::too_many_arguments, clippy::type_complexity)]
pub async fn run_matrix_test(
    app: AppHandle,
    pool: DbPool,
    run_id: String,
    ephemeral: EphemeralPersona,
    user_instruction: String,
    model_configs: Vec<TestModelConfig>,
    cancelled: Arc<std::sync::atomic::AtomicBool>,
    use_case_filter: Option<String>,
) {
    let persona = &ephemeral.persona;
    let tools = &ephemeral.tools;
    // Phase 0: Generate draft persona
    emit_lab_status(&app, "lab-matrix-status", &run_id, "drafting", None);

    let draft_prompt_text = build_draft_generation_prompt(persona, &user_instruction);
    let mut cli_args = prompt::build_cli_args(None, None);
    cli_args.args.push("--max-turns".to_string());
    cli_args.args.push("1".to_string());

    let draft_output = match spawn_cli_and_collect(&cli_args, &draft_prompt_text).await {
        Ok(o) => o,
        Err(e) => {
            let msg = format!("Draft generation failed: {e}");
            let now = chrono::Utc::now().to_rfc3339();
            let _ = matrix_repo::update_run_status(&pool, &run_id, "failed", None, None, Some(&msg), Some(&now));
            emit_lab_status(&app, "lab-matrix-status", &run_id, "failed", Some(&msg));
            return;
        }
    };

    // Parse draft JSON from output
    let (draft_structured_prompt, draft_change_summary) = match parse_draft_from_output(&draft_output) {
        Ok(v) => v,
        Err(e) => {
            let msg = format!("Failed to parse draft: {e}");
            let now = chrono::Utc::now().to_rfc3339();
            let _ = matrix_repo::update_run_status(&pool, &run_id, "failed", None, None, Some(&msg), Some(&now));
            emit_lab_status(&app, "lab-matrix-status", &run_id, "failed", Some(&msg));
            return;
        }
    };

    // Save draft to run
    let draft_json_str = serde_json::to_string(&draft_structured_prompt).unwrap_or_default();
    let _ = matrix_repo::update_run_draft(&pool, &run_id, &draft_json_str, &draft_change_summary);

    // Build draft persona variant
    let mut draft_persona = persona.clone();
    draft_persona.structured_prompt = Some(draft_json_str.clone());

    // Now run the same flow as A/B but with "current" and "draft" variants
    emit_lab_status(&app, "lab-matrix-status", &run_id, "generating", None);

    let scenarios = match generate_scenarios(persona, tools, use_case_filter.as_deref()).await {
        Ok(s) if s.is_empty() => {
            let now = chrono::Utc::now().to_rfc3339();
            let _ = matrix_repo::update_run_status(&pool, &run_id, "failed", None, None, Some("No test scenarios were generated"), Some(&now));
            emit_lab_status(&app, "lab-matrix-status", &run_id, "failed", Some("No test scenarios were generated"));
            return;
        }
        Ok(s) => s,
        Err(e) => {
            let msg = format!("Scenario generation failed: {e}");
            let now = chrono::Utc::now().to_rfc3339();
            let _ = matrix_repo::update_run_status(&pool, &run_id, "failed", None, None, Some(&msg), Some(&now));
            emit_lab_status(&app, "lab-matrix-status", &run_id, "failed", Some(&msg));
            return;
        }
    };

    let scenario_count = scenarios.len();
    let _ = matrix_repo::update_run_status(&pool, &run_id, "running", Some(scenario_count as i32), None, None, None);

    let _ = app.emit("lab-matrix-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "generated".into(),
        scenarios_count: Some(scenario_count), current: None, total: None,
        model_id: None, scenario_name: None, status: None, scores: None, summary: None, error: None,
        scenarios: None,
    });

    let variants: Vec<(&str, &Persona)> = vec![("current", persona), ("draft", &draft_persona)];
    let total = scenario_count * model_configs.len() * variants.len();
    let mut current_idx = 0usize;
    let results_tracker: Arc<Mutex<HashMap<String, Vec<(i32, i32, i32, f64, i64)>>>> = Arc::new(Mutex::new(HashMap::new()));

    for scenario in &scenarios {
        for model in &model_configs {
            for (variant_label, persona_variant) in &variants {
                if cancelled.load(std::sync::atomic::Ordering::Acquire) {
                    let _ = matrix_repo::update_run_status(&pool, &run_id, "cancelled", None, None, None, None);
                    emit_lab_status(&app, "lab-matrix-status", &run_id, "cancelled", None);
                    return;
                }

                current_idx += 1;

                let _ = app.emit("lab-matrix-status", TestRunStatusEvent {
                    run_id: run_id.clone(), phase: "executing".into(),
                    scenarios_count: Some(scenario_count), current: Some(current_idx), total: Some(total),
                    model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                    status: Some("running".into()), scores: None, summary: None, error: None,
                    scenarios: None,
                });

                let result = execute_scenario(persona_variant, tools, scenario, model).await;

                let (status, scores) = match &result {
                    Ok(r) => ("passed".to_string(), score_result(r, scenario)),
                    Err(e) => ("error".to_string(), ScoreResult {
                        tool_accuracy: 0, output_quality: 0, protocol_compliance: 0,
                        output_preview: Some(e.clone()), tool_calls_actual: None,
                        input_tokens: 0, output_tokens: 0, cost_usd: 0.0, duration_ms: 0,
                        error_message: Some(e.clone()),
                    }),
                };

                {
                    let key = format!("{}:{}", variant_label, model.id);
                    let mut tracker = results_tracker.lock().await;
                    tracker.entry(key).or_default().push((
                        scores.tool_accuracy, scores.output_quality, scores.protocol_compliance,
                        scores.cost_usd, scores.duration_ms,
                    ));
                }

                let _ = matrix_repo::create_result(&pool, &CreateMatrixResultInput {
                    run_id: run_id.clone(),
                    variant: variant_label.to_string(),
                    scenario_name: scenario.name.clone(),
                    model_id: model.id.clone(),
                    provider: model.provider.clone(),
                    status: status.clone(),
                    output_preview: scores.output_preview.clone(),
                    tool_calls_expected: scenario.expected_tool_sequence.as_ref().map(|v| serde_json::to_string(v).unwrap_or_default()),
                    tool_calls_actual: scores.tool_calls_actual.clone(),
                    tool_accuracy_score: Some(scores.tool_accuracy),
                    output_quality_score: Some(scores.output_quality),
                    protocol_compliance: Some(scores.protocol_compliance),
                    input_tokens: scores.input_tokens,
                    output_tokens: scores.output_tokens,
                    cost_usd: scores.cost_usd,
                    duration_ms: scores.duration_ms,
                    error_message: scores.error_message.clone(),
                });

                let _ = app.emit("lab-matrix-status", TestRunStatusEvent {
                    run_id: run_id.clone(), phase: "executing".into(),
                    scenarios_count: Some(scenario_count), current: Some(current_idx), total: Some(total),
                    model_id: Some(model.id.clone()), scenario_name: Some(scenario.name.clone()),
                    status: Some(status),
                    scores: Some(TestScores { tool_accuracy: Some(scores.tool_accuracy), output_quality: Some(scores.output_quality), protocol_compliance: Some(scores.protocol_compliance) }),
                    summary: None, error: scores.error_message,
                    scenarios: None,
                });
            }
        }
    }

    // Build matrix summary
    let tracker_data = results_tracker.lock().await;
    let mut summary_obj = serde_json::Map::new();
    for (key, results) in tracker_data.iter() {
        let count = results.len() as f64;
        let avg_ta = results.iter().map(|r| r.0 as f64).sum::<f64>() / count;
        let avg_oq = results.iter().map(|r| r.1 as f64).sum::<f64>() / count;
        let avg_pc = results.iter().map(|r| r.2 as f64).sum::<f64>() / count;
        let total_cost: f64 = results.iter().map(|r| r.3).sum();
        let composite = avg_ta * WEIGHT_TOOL_ACCURACY + avg_oq * WEIGHT_OUTPUT_QUALITY + avg_pc * WEIGHT_PROTOCOL_COMPLIANCE;
        summary_obj.insert(key.clone(), serde_json::json!({
            "avg_tool_accuracy": avg_ta.round() as i32,
            "avg_output_quality": avg_oq.round() as i32,
            "avg_protocol_compliance": avg_pc.round() as i32,
            "composite_score": composite.round() as i32,
            "total_cost_usd": (total_cost * 10000.0).round() / 10000.0,
            "scenarios_tested": count as i32,
        }));
    }
    drop(tracker_data);

    let summary = serde_json::Value::Object(summary_obj);
    let summary_str = serde_json::to_string(&summary).unwrap_or_default();
    let now = chrono::Utc::now().to_rfc3339();
    let _ = matrix_repo::update_run_status(&pool, &run_id, "completed", None, Some(&summary_str), None, Some(&now));

    let _ = app.emit("lab-matrix-status", TestRunStatusEvent {
        run_id: run_id.clone(), phase: "completed".into(),
        scenarios_count: Some(scenario_count), current: Some(total), total: Some(total),
        model_id: None, scenario_name: None, status: None, scores: None,
        summary: Some(summary), error: None,
        scenarios: None,
    });
}

// ── Matrix helpers ─────────────────────────────────────────────

fn build_draft_generation_prompt(persona: &Persona, user_instruction: &str) -> String {
    let sp_json = persona.structured_prompt.as_deref().unwrap_or("{}");

    format!(
        r#"# Persona Prompt Optimizer

You are a prompt engineering expert. Given the current persona prompt and a user's
improvement instruction, generate an optimized version of the structured prompt.

## Current Persona: {}
## Current Structured Prompt:
{}

## User's Instruction:
{}

## Output Format
Respond with ONLY a JSON object (no markdown fences, no extra text):
{{
  "structured_prompt": {{ "identity": "...", "instructions": "...", "toolGuidance": "...", "examples": "...", "errorHandling": "..." }},
  "change_summary": "Brief description of what was changed"
}}

Preserve all sections that don't need changes. Only modify what the user requested."#,
        persona.name, sp_json, user_instruction
    )
}

fn parse_draft_from_output(output: &str) -> Result<(serde_json::Value, String), String> {
    let trimmed = output.trim();

    // Try direct parse
    if let Ok(obj) = serde_json::from_str::<serde_json::Value>(trimmed) {
        if let Some(sp) = obj.get("structured_prompt") {
            let summary = obj.get("change_summary").and_then(|v| v.as_str()).unwrap_or("Draft generated").to_string();
            return Ok((sp.clone(), summary));
        }
    }

    // Try to extract JSON object from text
    if let Some(start) = trimmed.find('{') {
        if let Some(end) = trimmed.rfind('}') {
            let json_str = &trimmed[start..=end];
            if let Ok(obj) = serde_json::from_str::<serde_json::Value>(json_str) {
                if let Some(sp) = obj.get("structured_prompt") {
                    let summary = obj.get("change_summary").and_then(|v| v.as_str()).unwrap_or("Draft generated").to_string();
                    return Ok((sp.clone(), summary));
                }
            }
        }
    }

    Err(format!(
        "Failed to parse draft from output. Raw output (first 500 chars): {}",
        &trimmed[..trimmed.len().min(500)]
    ))
}
