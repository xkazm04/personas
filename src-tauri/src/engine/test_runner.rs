use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tauri::{AppHandle, Emitter};
use tokio::io::{AsyncBufReadExt, AsyncWriteExt, BufReader};
use tokio::process::Command;
use tokio::sync::Mutex;

use serde::{Deserialize, Serialize};

use crate::db::models::{CreateTestResultInput, Persona, PersonaToolDefinition};
use crate::db::repos::execution::test_runs as repo;
use crate::db::DbPool;

use super::parser;
use super::prompt;
use super::types::*;

// ── Types ──────────────────────────────────────────────────────

/// Model configuration for a test run, passed from the frontend.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestModelConfig {
    pub id: String,
    pub provider: String,
    pub model: Option<String>,
    pub base_url: Option<String>,
    pub auth_token: Option<String>,
}

/// A test scenario generated by the coordinator LLM.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestScenario {
    pub name: String,
    pub description: String,
    pub input_data: Option<serde_json::Value>,
    pub mock_tools: Vec<MockToolResponse>,
    pub expected_behavior: String,
    pub expected_tool_sequence: Option<Vec<String>>,
    pub expected_protocols: Option<Vec<String>>,
}

/// A mock tool response within a scenario.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MockToolResponse {
    pub tool_name: String,
    pub description: Option<String>,
    pub mock_response: serde_json::Value,
}

/// Tauri event payload for test run progress.
#[derive(Debug, Clone, Serialize)]
pub struct TestRunStatusEvent {
    pub run_id: String,
    pub phase: String,
    pub scenarios_count: Option<usize>,
    pub current: Option<usize>,
    pub total: Option<usize>,
    pub model_id: Option<String>,
    pub scenario_name: Option<String>,
    pub status: Option<String>,
    pub scores: Option<TestScores>,
    pub summary: Option<serde_json::Value>,
    pub error: Option<String>,
}

#[derive(Debug, Clone, Serialize)]
pub struct TestScores {
    pub tool_accuracy: Option<i32>,
    pub output_quality: Option<i32>,
    pub protocol_compliance: Option<i32>,
}

// ── Main entry point ───────────────────────────────────────────

/// Run a full test session: generate scenarios, execute across models, score, summarize.
#[allow(clippy::too_many_arguments)]
pub async fn run_test(
    app: AppHandle,
    pool: DbPool,
    run_id: String,
    persona: Persona,
    tools: Vec<PersonaToolDefinition>,
    model_configs: Vec<TestModelConfig>,
    _log_dir: PathBuf,
    cancelled: Arc<std::sync::atomic::AtomicBool>,
) {
    // Phase 1: Generate scenarios
    emit_status(&app, &run_id, "generating", None);

    let scenarios = match generate_scenarios(&persona, &tools).await {
        Ok(s) if s.is_empty() => {
            finish_with_error(&app, &pool, &run_id, "No test scenarios were generated");
            return;
        }
        Ok(s) => s,
        Err(e) => {
            finish_with_error(&app, &pool, &run_id, &format!("Scenario generation failed: {e}"));
            return;
        }
    };

    let scenario_count = scenarios.len();
    let _ = repo::update_run_status(
        &pool,
        &run_id,
        "running",
        Some(scenario_count as i32),
        None,
        None,
        None,
    );

    let _ = app.emit(
        "test-run-status",
        TestRunStatusEvent {
            run_id: run_id.clone(),
            phase: "generated".into(),
            scenarios_count: Some(scenario_count),
            current: None,
            total: None,
            model_id: None,
            scenario_name: None,
            status: None,
            scores: None,
            summary: None,
            error: None,
        },
    );

    // Phase 2: Execute each scenario × model combination
    let total = scenario_count * model_configs.len();
    let mut current = 0usize;

    // Track results for summary
    let results_tracker: Arc<Mutex<Vec<(String, i32, i32, i32, f64, i64)>>> =
        Arc::new(Mutex::new(Vec::new()));

    for scenario in &scenarios {
        for model in &model_configs {
            // Check cancellation
            if cancelled.load(std::sync::atomic::Ordering::Acquire) {
                let _ = repo::update_run_status(
                    &pool, &run_id, "cancelled", None, None, None, None,
                );
                emit_status(&app, &run_id, "cancelled", None);
                return;
            }

            current += 1;

            let _ = app.emit(
                "test-run-status",
                TestRunStatusEvent {
                    run_id: run_id.clone(),
                    phase: "executing".into(),
                    scenarios_count: Some(scenario_count),
                    current: Some(current),
                    total: Some(total),
                    model_id: Some(model.id.clone()),
                    scenario_name: Some(scenario.name.clone()),
                    status: Some("running".into()),
                    scores: None,
                    summary: None,
                    error: None,
                },
            );

            let result = execute_scenario(&persona, &tools, scenario, model).await;

            let (status, scores) = match &result {
                Ok(r) => {
                    let s = score_result(r, scenario);
                    ("passed".to_string(), s)
                }
                Err(e) => (
                    "error".to_string(),
                    ScoreResult {
                        tool_accuracy: 0,
                        output_quality: 0,
                        protocol_compliance: 0,
                        output_preview: Some(e.clone()),
                        tool_calls_actual: None,
                        input_tokens: 0,
                        output_tokens: 0,
                        cost_usd: 0.0,
                        duration_ms: 0,
                        error_message: Some(e.clone()),
                    },
                ),
            };

            // Track for summary
            {
                let mut tracker = results_tracker.lock().await;
                tracker.push((
                    model.id.clone(),
                    scores.tool_accuracy,
                    scores.output_quality,
                    scores.protocol_compliance,
                    scores.cost_usd,
                    scores.duration_ms,
                ));
            }

            // Write result to DB
            let _ = repo::create_result(
                &pool,
                &CreateTestResultInput {
                    test_run_id: run_id.clone(),
                    scenario_name: scenario.name.clone(),
                    model_id: model.id.clone(),
                    provider: model.provider.clone(),
                    status: status.clone(),
                    output_preview: scores.output_preview.clone(),
                    tool_calls_expected: scenario
                        .expected_tool_sequence
                        .as_ref()
                        .map(|v| serde_json::to_string(v).unwrap_or_default()),
                    tool_calls_actual: scores.tool_calls_actual.clone(),
                    tool_accuracy_score: Some(scores.tool_accuracy),
                    output_quality_score: Some(scores.output_quality),
                    protocol_compliance: Some(scores.protocol_compliance),
                    input_tokens: scores.input_tokens,
                    output_tokens: scores.output_tokens,
                    cost_usd: scores.cost_usd,
                    duration_ms: scores.duration_ms,
                    error_message: scores.error_message.clone(),
                },
            );

            // Emit progress
            let _ = app.emit(
                "test-run-status",
                TestRunStatusEvent {
                    run_id: run_id.clone(),
                    phase: "executing".into(),
                    scenarios_count: Some(scenario_count),
                    current: Some(current),
                    total: Some(total),
                    model_id: Some(model.id.clone()),
                    scenario_name: Some(scenario.name.clone()),
                    status: Some(status),
                    scores: Some(TestScores {
                        tool_accuracy: Some(scores.tool_accuracy),
                        output_quality: Some(scores.output_quality),
                        protocol_compliance: Some(scores.protocol_compliance),
                    }),
                    summary: None,
                    error: scores.error_message,
                },
            );
        }
    }

    // Phase 3: Build summary
    let summary = build_summary(&results_tracker, &model_configs).await;
    let summary_str = serde_json::to_string(&summary).unwrap_or_default();
    let now = chrono::Utc::now().to_rfc3339();

    let _ = repo::update_run_status(
        &pool,
        &run_id,
        "completed",
        None,
        Some(&summary_str),
        None,
        Some(&now),
    );

    let _ = app.emit(
        "test-run-status",
        TestRunStatusEvent {
            run_id: run_id.clone(),
            phase: "completed".into(),
            scenarios_count: Some(scenario_count),
            current: Some(total),
            total: Some(total),
            model_id: None,
            scenario_name: None,
            status: None,
            scores: None,
            summary: Some(summary),
            error: None,
        },
    );
}

// ── Phase 1: Generate scenarios ────────────────────────────────

async fn generate_scenarios(
    persona: &Persona,
    tools: &[PersonaToolDefinition],
) -> Result<Vec<TestScenario>, String> {
    let coordinator_prompt = build_coordinator_prompt(persona, tools);

    let mut cli_args = prompt::build_default_cli_args();
    // Limit to 1 turn — we just want the JSON output
    cli_args.args.push("--max-turns".to_string());
    cli_args.args.push("1".to_string());

    let output = spawn_cli_and_collect(&cli_args, &coordinator_prompt).await?;

    // Extract JSON array from the output
    parse_scenarios_from_output(&output)
}

fn build_coordinator_prompt(persona: &Persona, tools: &[PersonaToolDefinition]) -> String {
    let mut p = String::new();

    p.push_str("# Test Scenario Generator\n\n");
    p.push_str("You are a QA engineer generating test scenarios for an AI agent.\n\n");

    // Agent identity
    p.push_str("## Agent Under Test\n");
    p.push_str(&format!("**Name**: {}\n", persona.name));
    if let Some(ref desc) = persona.description {
        if !desc.is_empty() {
            p.push_str(&format!("**Description**: {}\n", desc));
        }
    }
    p.push('\n');

    // Agent prompt
    p.push_str("### Agent Prompt\n");
    if let Some(ref sp_json) = persona.structured_prompt {
        if let Ok(sp) = serde_json::from_str::<serde_json::Value>(sp_json) {
            for section in &["identity", "instructions", "toolGuidance", "examples", "errorHandling"] {
                if let Some(val) = sp.get(section).and_then(|v| v.as_str()) {
                    if !val.is_empty() {
                        p.push_str(&format!("**{}**: {}\n\n", section, val));
                    }
                }
            }
        }
    } else if !persona.system_prompt.is_empty() {
        p.push_str(&persona.system_prompt);
        p.push_str("\n\n");
    }

    // Available tools
    if !tools.is_empty() {
        p.push_str("### Available Tools\n");
        for tool in tools {
            p.push_str(&format!("- **{}** ({}): {}\n", tool.name, tool.category, tool.description));
            if let Some(ref schema) = tool.input_schema {
                p.push_str(&format!("  Input schema: {}\n", schema));
            }
        }
        p.push('\n');
    }

    // Task instructions
    p.push_str("## Your Task\n");
    p.push_str("Generate 3-5 realistic test scenarios for this agent. Each scenario must:\n");
    p.push_str("1. Represent a plausible real-world situation this agent would handle\n");
    p.push_str("2. Include realistic mock tool responses for every tool the agent might call\n");
    p.push_str("3. Describe the expected behavior and output\n\n");

    // Output format
    p.push_str("## Output Format\n");
    p.push_str("Respond with ONLY a JSON array (no markdown fences, no extra text):\n");
    p.push_str(r#"[{
  "name": "Short scenario name",
  "description": "What this scenario tests",
  "input_data": {},
  "mock_tools": [{
    "tool_name": "tool_name_here",
    "description": "What this mock simulates",
    "mock_response": {}
  }],
  "expected_behavior": "Description of what a good response looks like",
  "expected_tool_sequence": ["tool1", "tool2"],
  "expected_protocols": ["user_message"]
}]"#);

    p
}

fn parse_scenarios_from_output(output: &str) -> Result<Vec<TestScenario>, String> {
    // Try to find a JSON array in the output
    // The output may contain other text before/after the JSON
    let trimmed = output.trim();

    // Try direct parse first
    if let Ok(scenarios) = serde_json::from_str::<Vec<TestScenario>>(trimmed) {
        return Ok(scenarios);
    }

    // Try to extract JSON array from the text
    if let Some(start) = trimmed.find('[') {
        if let Some(end) = trimmed.rfind(']') {
            let json_str = &trimmed[start..=end];
            if let Ok(scenarios) = serde_json::from_str::<Vec<TestScenario>>(json_str) {
                return Ok(scenarios);
            }
        }
    }

    Err(format!(
        "Failed to parse test scenarios from coordinator output. Raw output (first 500 chars): {}",
        &trimmed[..trimmed.len().min(500)]
    ))
}

// ── Phase 2: Execute scenario with a specific model ────────────

struct ScoreResult {
    tool_accuracy: i32,
    output_quality: i32,
    protocol_compliance: i32,
    output_preview: Option<String>,
    tool_calls_actual: Option<String>,
    input_tokens: i64,
    output_tokens: i64,
    cost_usd: f64,
    duration_ms: i64,
    error_message: Option<String>,
}

async fn execute_scenario(
    persona: &Persona,
    tools: &[PersonaToolDefinition],
    scenario: &TestScenario,
    model: &TestModelConfig,
) -> Result<ExecutionOutput, String> {
    // Build the base prompt
    let base_prompt = prompt::assemble_prompt(persona, tools, scenario.input_data.as_ref(), None);

    // Inject sandbox mock section before the EXECUTE NOW section
    let sandbox_section = build_sandbox_section(&scenario.mock_tools);
    let final_prompt = inject_sandbox_into_prompt(&base_prompt, &sandbox_section);

    // Build CLI args for this model
    let model_profile = ModelProfile {
        model: model.model.clone(),
        provider: Some(model.provider.clone()),
        base_url: model.base_url.clone(),
        auth_token: model.auth_token.clone(),
    };

    let mut cli_args = prompt::build_default_cli_args();

    // Apply model
    if let Some(ref m) = model_profile.model {
        if !m.is_empty() {
            cli_args.args.push("--model".to_string());
            cli_args.args.push(m.clone());
        }
    }

    // Apply provider env vars
    match model_profile.provider.as_deref() {
        Some("ollama") => {
            if let Some(ref base_url) = model_profile.base_url {
                cli_args.env_overrides.push(("OLLAMA_BASE_URL".to_string(), base_url.clone()));
            }
            if let Some(ref auth_token) = model_profile.auth_token {
                if !auth_token.is_empty() {
                    cli_args.env_overrides.push(("OLLAMA_API_KEY".to_string(), auth_token.clone()));
                }
            }
            cli_args.env_removals.push("ANTHROPIC_API_KEY".to_string());
        }
        Some("litellm") => {
            if let Some(ref base_url) = model_profile.base_url {
                cli_args.env_overrides.push(("ANTHROPIC_BASE_URL".to_string(), base_url.clone()));
            }
            if let Some(ref auth_token) = model_profile.auth_token {
                if !auth_token.is_empty() {
                    cli_args.env_overrides.push(("ANTHROPIC_AUTH_TOKEN".to_string(), auth_token.clone()));
                }
            }
            cli_args.env_removals.push("ANTHROPIC_API_KEY".to_string());
        }
        Some("custom") => {
            if let Some(ref base_url) = model_profile.base_url {
                cli_args.env_overrides.push(("OPENAI_BASE_URL".to_string(), base_url.clone()));
            }
            if let Some(ref auth_token) = model_profile.auth_token {
                cli_args.env_overrides.push(("OPENAI_API_KEY".to_string(), auth_token.clone()));
            }
            cli_args.env_removals.push("ANTHROPIC_API_KEY".to_string());
        }
        _ => {}
    }

    // Limit turns for sandbox testing
    cli_args.args.push("--max-turns".to_string());
    cli_args.args.push("3".to_string());

    // Run the CLI and collect structured output
    spawn_cli_and_collect_structured(&cli_args, &final_prompt).await
}

fn build_sandbox_section(mock_tools: &[MockToolResponse]) -> String {
    let mut section = String::new();
    section.push_str("\n## SANDBOX TESTING MODE — Simulated Tool Environment\n");
    section.push_str("You are running in test mode. Do NOT call actual tools.\n");
    section.push_str("Instead, use these simulated tool responses as if the tools returned them:\n\n");

    for mock in mock_tools {
        section.push_str(&format!("### Simulated response for `{}`\n", mock.tool_name));
        if let Some(ref desc) = mock.description {
            section.push_str(&format!("Context: {}\n", desc));
        }
        section.push_str("Assume it returns:\n```json\n");
        section.push_str(&serde_json::to_string_pretty(&mock.mock_response).unwrap_or_default());
        section.push_str("\n```\n\n");
    }

    section.push_str("Process these simulated results exactly as you would real tool responses.\n");
    section.push_str("Complete your full workflow and emit all appropriate protocol messages.\n");
    section.push_str("Do NOT mention that you are in test mode.\n\n");

    section
}

fn inject_sandbox_into_prompt(base_prompt: &str, sandbox_section: &str) -> String {
    // Insert the sandbox section before "## EXECUTE NOW" if it exists
    if let Some(pos) = base_prompt.find("## EXECUTE NOW") {
        let mut result = String::with_capacity(base_prompt.len() + sandbox_section.len());
        result.push_str(&base_prompt[..pos]);
        result.push_str(sandbox_section);
        result.push_str(&base_prompt[pos..]);
        result
    } else {
        // Fallback: append at end
        format!("{}\n{}", base_prompt, sandbox_section)
    }
}

// ── Scoring ────────────────────────────────────────────────────

fn score_result(output: &ExecutionOutput, scenario: &TestScenario) -> ScoreResult {
    let tool_accuracy = score_tool_accuracy(&output.tool_calls, &scenario.expected_tool_sequence);
    let output_quality = score_output_quality(&output.assistant_text, &scenario.expected_behavior);
    let protocol_compliance = score_protocol_compliance(&output.assistant_text, &scenario.expected_protocols);

    let tool_calls_json = if output.tool_calls.is_empty() {
        None
    } else {
        Some(serde_json::to_string(&output.tool_calls).unwrap_or_default())
    };

    let preview = if output.assistant_text.len() > 2000 {
        Some(output.assistant_text[..2000].to_string())
    } else if output.assistant_text.is_empty() {
        None
    } else {
        Some(output.assistant_text.clone())
    };

    ScoreResult {
        tool_accuracy,
        output_quality,
        protocol_compliance,
        output_preview: preview,
        tool_calls_actual: tool_calls_json,
        input_tokens: output.input_tokens as i64,
        output_tokens: output.output_tokens as i64,
        cost_usd: output.cost_usd,
        duration_ms: output.duration_ms as i64,
        error_message: output.error.clone(),
    }
}

fn score_tool_accuracy(actual: &[String], expected: &Option<Vec<String>>) -> i32 {
    let expected = match expected {
        Some(e) if !e.is_empty() => e,
        _ => return if actual.is_empty() { 100 } else { 50 }, // No expectations
    };

    if expected.is_empty() {
        return 100;
    }

    // Check what fraction of expected tools were referenced
    let mut matched = 0;
    for exp in expected {
        if actual.iter().any(|a| a == exp) {
            matched += 1;
        }
    }

    let recall = (matched as f64 / expected.len() as f64) * 100.0;

    // Penalize for extra unexpected tool calls (but mildly)
    let extra = actual.len().saturating_sub(expected.len());
    let penalty = (extra as f64 * 5.0).min(20.0);

    (recall - penalty).max(0.0).min(100.0) as i32
}

fn score_output_quality(output: &str, expected_behavior: &str) -> i32 {
    if output.is_empty() {
        return 0;
    }
    if expected_behavior.is_empty() {
        return 50;
    }

    // Extract key terms from expected_behavior and check for presence
    let keywords: Vec<&str> = expected_behavior
        .split_whitespace()
        .filter(|w| w.len() > 3) // skip short words
        .collect();

    if keywords.is_empty() {
        return 50;
    }

    let output_lower = output.to_lowercase();
    let mut found = 0;
    for kw in &keywords {
        if output_lower.contains(&kw.to_lowercase()) {
            found += 1;
        }
    }

    let score = (found as f64 / keywords.len() as f64) * 100.0;
    score.min(100.0) as i32
}

fn score_protocol_compliance(output: &str, expected_protocols: &Option<Vec<String>>) -> i32 {
    let expected = match expected_protocols {
        Some(e) if !e.is_empty() => e,
        _ => return 100, // No protocol expectations
    };

    let mut found = 0;
    for proto in expected {
        let pattern = format!("\"{}\":", proto);
        let alt_pattern = format!("{{{}", proto);
        if output.contains(&pattern) || output.contains(&alt_pattern) {
            found += 1;
        }
    }

    if expected.is_empty() {
        100
    } else {
        ((found as f64 / expected.len() as f64) * 100.0) as i32
    }
}

// ── Summary builder ────────────────────────────────────────────

async fn build_summary(
    results: &Arc<Mutex<Vec<(String, i32, i32, i32, f64, i64)>>>,
    models: &[TestModelConfig],
) -> serde_json::Value {
    let data = results.lock().await;

    let mut per_model: HashMap<String, Vec<(i32, i32, i32, f64, i64)>> = HashMap::new();
    for (model_id, ta, oq, pc, cost, duration) in data.iter() {
        per_model
            .entry(model_id.clone())
            .or_default()
            .push((*ta, *oq, *pc, *cost, *duration));
    }

    let mut rankings: Vec<serde_json::Value> = Vec::new();

    for model in models {
        if let Some(results) = per_model.get(&model.id) {
            let count = results.len() as f64;
            let avg_ta = results.iter().map(|r| r.0 as f64).sum::<f64>() / count;
            let avg_oq = results.iter().map(|r| r.1 as f64).sum::<f64>() / count;
            let avg_pc = results.iter().map(|r| r.2 as f64).sum::<f64>() / count;
            let total_cost: f64 = results.iter().map(|r| r.3).sum();
            let avg_duration = results.iter().map(|r| r.4 as f64).sum::<f64>() / count;

            let composite = avg_ta * 0.4 + avg_oq * 0.4 + avg_pc * 0.2;
            let value_score = if total_cost > 0.0 {
                composite / (total_cost * 1000.0 + 1.0) * 100.0
            } else {
                composite // Free models get composite as value
            };

            rankings.push(serde_json::json!({
                "model_id": model.id,
                "provider": model.provider,
                "avg_tool_accuracy": avg_ta.round() as i32,
                "avg_output_quality": avg_oq.round() as i32,
                "avg_protocol_compliance": avg_pc.round() as i32,
                "composite_score": composite.round() as i32,
                "total_cost_usd": (total_cost * 10000.0).round() / 10000.0,
                "avg_duration_ms": avg_duration.round() as i64,
                "value_score": value_score.round() as i32,
                "scenarios_tested": count as i32,
            }));
        }
    }

    // Sort by composite score descending
    rankings.sort_by(|a, b| {
        let sa = a.get("composite_score").and_then(|v| v.as_i64()).unwrap_or(0);
        let sb = b.get("composite_score").and_then(|v| v.as_i64()).unwrap_or(0);
        sb.cmp(&sa)
    });

    let best_model = rankings
        .first()
        .and_then(|r| r.get("model_id"))
        .and_then(|v| v.as_str())
        .unwrap_or("unknown")
        .to_string();

    let best_value = rankings
        .iter()
        .max_by_key(|r| r.get("value_score").and_then(|v| v.as_i64()).unwrap_or(0))
        .and_then(|r| r.get("model_id"))
        .and_then(|v| v.as_str())
        .unwrap_or("unknown")
        .to_string();

    serde_json::json!({
        "best_quality_model": best_model,
        "best_value_model": best_value,
        "rankings": rankings,
    })
}

// ── CLI helpers ────────────────────────────────────────────────

/// Structured output from a CLI execution.
struct ExecutionOutput {
    assistant_text: String,
    tool_calls: Vec<String>,
    input_tokens: u64,
    output_tokens: u64,
    cost_usd: f64,
    duration_ms: u64,
    error: Option<String>,
}

/// Spawn Claude CLI, pipe prompt to stdin, collect all output as a plain string.
/// Used for the coordinator (scenario generation).
async fn spawn_cli_and_collect(cli_args: &CliArgs, prompt_text: &str) -> Result<String, String> {
    let exec_dir = std::env::temp_dir().join(format!(
        "personas-test-coord-{}",
        uuid::Uuid::new_v4()
    ));
    std::fs::create_dir_all(&exec_dir)
        .map_err(|e| format!("Failed to create temp dir: {e}"))?;

    let mut cmd = Command::new(&cli_args.command);
    cmd.args(&cli_args.args)
        .current_dir(&exec_dir)
        .stdin(std::process::Stdio::piped())
        .stdout(std::process::Stdio::piped())
        .stderr(std::process::Stdio::null());

    #[cfg(windows)]
    {
        #[allow(unused_imports)]
        use std::os::windows::process::CommandExt;
        cmd.creation_flags(0x08000000);
    }

    for key in &cli_args.env_removals {
        cmd.env_remove(key);
    }
    for (key, val) in &cli_args.env_overrides {
        cmd.env(key, val);
    }

    let mut child = cmd.spawn().map_err(|e| format!("Failed to spawn CLI: {e}"))?;

    if let Some(mut stdin) = child.stdin.take() {
        let _ = stdin.write_all(prompt_text.as_bytes()).await;
        let _ = stdin.shutdown().await;
    }

    let stdout = child.stdout.take().ok_or("No stdout")?;
    let mut reader = BufReader::new(stdout).lines();
    let mut assistant_text = String::new();

    let timeout = tokio::time::Duration::from_secs(120);
    let result = tokio::time::timeout(timeout, async {
        while let Ok(Some(line)) = reader.next_line().await {
            let (line_type, _) = parser::parse_stream_line(&line);
            if let StreamLineType::AssistantText { text } = line_type {
                assistant_text.push_str(&text);
                assistant_text.push('\n');
            }
        }
    })
    .await;

    let _ = child.wait().await;
    let _ = std::fs::remove_dir_all(&exec_dir);

    if result.is_err() {
        return Err("Coordinator CLI timed out after 120 seconds".into());
    }

    Ok(assistant_text)
}

/// Spawn Claude CLI, pipe prompt to stdin, collect structured execution output.
/// Used for the per-model persona execution.
async fn spawn_cli_and_collect_structured(
    cli_args: &CliArgs,
    prompt_text: &str,
) -> Result<ExecutionOutput, String> {
    let exec_dir = std::env::temp_dir().join(format!(
        "personas-test-exec-{}",
        uuid::Uuid::new_v4()
    ));
    std::fs::create_dir_all(&exec_dir)
        .map_err(|e| format!("Failed to create temp dir: {e}"))?;

    let mut cmd = Command::new(&cli_args.command);
    cmd.args(&cli_args.args)
        .current_dir(&exec_dir)
        .stdin(std::process::Stdio::piped())
        .stdout(std::process::Stdio::piped())
        .stderr(std::process::Stdio::null());

    #[cfg(windows)]
    {
        #[allow(unused_imports)]
        use std::os::windows::process::CommandExt;
        cmd.creation_flags(0x08000000);
    }

    for key in &cli_args.env_removals {
        cmd.env_remove(key);
    }
    for (key, val) in &cli_args.env_overrides {
        cmd.env(key, val);
    }

    let start = std::time::Instant::now();
    let mut child = cmd.spawn().map_err(|e| format!("Failed to spawn CLI: {e}"))?;

    if let Some(mut stdin) = child.stdin.take() {
        let _ = stdin.write_all(prompt_text.as_bytes()).await;
        let _ = stdin.shutdown().await;
    }

    let stdout = child.stdout.take().ok_or("No stdout")?;
    let mut reader = BufReader::new(stdout).lines();

    let mut assistant_text = String::new();
    let mut tool_calls: Vec<String> = Vec::new();
    let mut metrics = ExecutionMetrics::default();

    let timeout = tokio::time::Duration::from_secs(180);
    let stream_result = tokio::time::timeout(timeout, async {
        while let Ok(Some(line)) = reader.next_line().await {
            let (line_type, _) = parser::parse_stream_line(&line);

            match line_type {
                StreamLineType::AssistantText { text } => {
                    assistant_text.push_str(&text);
                    assistant_text.push('\n');
                }
                StreamLineType::AssistantToolUse { tool_name, .. } => {
                    tool_calls.push(tool_name);
                }
                StreamLineType::Result { .. } => {
                    parser::update_metrics_from_result(&mut metrics, &line_type);
                }
                _ => {}
            }
        }
    })
    .await;

    let exit = child.wait().await;
    let duration_ms = start.elapsed().as_millis() as u64;
    let _ = std::fs::remove_dir_all(&exec_dir);

    let error = if stream_result.is_err() {
        Some("Execution timed out after 180 seconds".to_string())
    } else if let Ok(status) = exit {
        if !status.success() {
            Some(format!("CLI exited with code {}", status.code().unwrap_or(-1)))
        } else {
            None
        }
    } else {
        None
    };

    Ok(ExecutionOutput {
        assistant_text,
        tool_calls,
        input_tokens: metrics.input_tokens,
        output_tokens: metrics.output_tokens,
        cost_usd: metrics.cost_usd,
        duration_ms,
        error,
    })
}

// ── Utility helpers ────────────────────────────────────────────

fn emit_status(app: &AppHandle, run_id: &str, phase: &str, error: Option<&str>) {
    let _ = app.emit(
        "test-run-status",
        TestRunStatusEvent {
            run_id: run_id.to_string(),
            phase: phase.to_string(),
            scenarios_count: None,
            current: None,
            total: None,
            model_id: None,
            scenario_name: None,
            status: None,
            scores: None,
            summary: None,
            error: error.map(|s| s.to_string()),
        },
    );
}

fn finish_with_error(app: &AppHandle, pool: &DbPool, run_id: &str, error: &str) {
    let now = chrono::Utc::now().to_rfc3339();
    let _ = repo::update_run_status(pool, run_id, "failed", None, None, Some(error), Some(&now));
    emit_status(app, run_id, "failed", Some(error));
}
