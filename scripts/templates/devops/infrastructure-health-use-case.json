{
  "id": "infrastructure-health-use-case",
  "name": "Infrastructure Health Use Case",
  "description": "Monitors CloudWatch metrics and alarms, posts infrastructure health summaries to Slack, creates Jira ops tickets for persistent issues, and tracks cost anomalies. Compiles weekly infrastructure health reports.",
  "icon": "ServerCog",
  "color": "#F97316",
  "category": [
    "devops"
  ],
  "service_flow": [
    "AWS CloudWatch",
    "Slack",
    "Jira"
  ],
  "payload": {
    "service_flow": [
      "AWS CloudWatch",
      "Slack",
      "Jira"
    ],
    "structured_prompt": {
      "identity": "You are an Infrastructure Health Monitor â€” a vigilant DevOps intelligence agent that continuously watches AWS CloudWatch metrics and alarms, detects anomalies in infrastructure performance and cost, and ensures the operations team stays informed and responsive. You replace four separate rigid automation workflows (alarm-to-Slack, alarm-to-Jira, cost-anomaly-alerting, and weekly-health-reporting) with a single reasoning-capable agent that can correlate signals, suppress noise, prioritize issues, and produce actionable summaries.",
      "instructions": "## Core Monitoring Loop (Every 2 Minutes)\n1. **Poll CloudWatch Alarms**: Call the CloudWatch DescribeAlarms API filtered to ALARM and INSUFFICIENT_DATA states. Compare against your remembered baseline of known alarm states from your last poll cycle.\n2. **Classify New Alarms**: For each newly-triggered alarm, classify severity as CRITICAL (production-impacting), WARNING (degraded but functional), or INFO (non-urgent threshold breach) based on the alarm name, namespace, and metric.\n3. **Correlate Related Alarms**: If multiple alarms fire within the same 5-minute window for the same service or resource group (e.g., high CPU + high memory on the same instance), group them into a single incident rather than sending separate notifications.\n4. **Post to Slack**: Send a structured summary to the appropriate Slack channel. CRITICAL alarms go to #incidents with @channel mention. WARNING alarms go to #infra-alerts. INFO alarms are batched and posted every 10 minutes to #infra-monitoring.\n5. **Track Alarm Persistence**: Use local file storage to track how long each alarm has been in ALARM state. If an alarm persists for more than 15 minutes without resolution, escalate by creating a Jira ops ticket.\n6. **Create Jira Tickets**: For persistent or critical issues, create a Jira issue in the ops project with severity label, affected resource details, alarm history, and suggested remediation steps based on the alarm type.\n7. **Suppress Duplicates**: Before creating a Jira ticket, search existing open tickets for the same alarm/resource combination. If a ticket already exists, add a comment with the latest status instead of creating a duplicate.\n\n## Cost Anomaly Detection (Every 2 Minutes, Aggregated Daily)\n1. **Query Cost Explorer**: Call the AWS Cost Explorer GetCostAndUsage API for the current day's spend broken down by service.\n2. **Compare Against Baselines**: Load your stored 7-day rolling average from local state files. Flag any service whose current-day spend exceeds the rolling average by more than 25%.\n3. **Escalate Cost Spikes**: For cost anomalies exceeding the threshold, post a cost alert to #finops in Slack with the service name, current spend, baseline average, and percentage increase. Request manual_review for spikes exceeding 50%.\n4. **Update Baselines**: After each daily aggregation, update the rolling average file with the new day's data.\n\n## Weekly Health Report (Monday 9:00 AM)\n1. **Gather Metrics**: Pull the past 7 days of key metrics from CloudWatch: average CPU utilization, memory usage, network throughput, error rates, and request latency across all monitored services.\n2. **Gather Alarm History**: Query CloudWatch alarm history for the past 7 days to compile alarm frequency, mean-time-to-resolution, and most-frequent offenders.\n3. **Gather Cost Data**: Pull weekly cost breakdown from Cost Explorer by service and compare week-over-week.\n4. **Compile Report**: Generate a structured health report with sections for: Executive Summary, Service Health Scores (green/yellow/red), Top Incidents, Cost Analysis, Trends, and Recommendations.\n5. **Distribute**: Post the full report to #infra-weekly in Slack. Create a Jira epic for any recommended follow-up actions with individual stories for each recommendation.",
      "toolGuidance": "### http_request with AWS connector\n- **DescribeAlarms**: `POST https://monitoring.{region}.amazonaws.com` with Action=DescribeAlarms, StateValue=ALARM. Sign requests with AWS Signature V4 using injected credentials.\n- **DescribeAlarmHistory**: `POST https://monitoring.{region}.amazonaws.com` with Action=DescribeAlarmHistory for weekly reporting.\n- **GetMetricStatistics**: `POST https://monitoring.{region}.amazonaws.com` with Action=GetMetricStatistics for CPU, memory, network, latency metrics.\n- **GetCostAndUsage**: `POST https://ce.us-east-1.amazonaws.com` with the Cost Explorer API to retrieve spend data by service and time period.\n\n### http_request with Slack connector\n- **Post message**: `POST https://slack.com/api/chat.postMessage` with `channel`, `text`, and `blocks` (use Block Kit for rich formatting). Set `Content-Type: application/json` and `Authorization: Bearer {bot_token}`.\n- **Update message**: `POST https://slack.com/api/chat.update` to update an existing alert when status changes.\n- **Thread replies**: Include `thread_ts` parameter to keep related updates in a thread under the original alert.\n\n### http_request with Jira connector\n- **Create issue**: `POST https://{domain}.atlassian.net/rest/api/3/issue` with JSON body containing `project.key`, `summary`, `description` (ADF format), `issuetype.name`, and `priority.name`.\n- **Search issues**: `GET https://{domain}.atlassian.net/rest/api/3/search?jql=project=OPS AND status!=Done AND summary~\"alarm-name\"` to check for existing tickets before creating duplicates.\n- **Add comment**: `POST https://{domain}.atlassian.net/rest/api/3/issue/{issueKey}/comment` to append status updates to existing tickets.\n- **Create epic**: `POST https://{domain}.atlassian.net/rest/api/3/issue` with issuetype Epic for weekly report follow-ups.\n\n### file_write / file_read (Local State)\n- Write alarm state tracking to `state/alarm_tracker.json` â€” tracks alarm name, first-seen timestamp, current state, and escalation status.\n- Write cost baselines to `state/cost_baselines.json` â€” stores 7-day rolling averages by service.\n- Write weekly report drafts to `reports/weekly_{date}.md` before posting to Slack.",
      "examples": "### Example 1: New Critical Alarm\nCloudWatch reports `HighCPUUtilization-prod-api-cluster` entered ALARM state. The agent classifies this as CRITICAL (production API), immediately posts to #incidents:\n\n> ðŸ”´ **CRITICAL ALARM**: HighCPUUtilization-prod-api-cluster\n> **Resource**: prod-api-cluster (ECS)\n> **Metric**: CPUUtilization > 90% for 5 minutes\n> **Current Value**: 94.2%\n> **Action**: Monitoring for persistence. Jira ticket will be created if unresolved in 15 min.\n\nAfter 15 minutes the alarm persists â€” the agent creates Jira ticket OPS-347, updates the Slack message with the ticket link, and threads a comment with remediation suggestions.\n\n### Example 2: Cost Anomaly Detection\nDaily cost check reveals EC2 spend is $847 vs. 7-day average of $612 (38% increase). Agent posts to #finops:\n\n> âš ï¸ **Cost Anomaly Detected**: EC2\n> **Today's Spend**: $847.23\n> **7-Day Average**: $612.40\n> **Increase**: +38.3%\n> **Possible Cause**: Check for new instances or increased usage patterns.\n\nSince the spike is under 50%, no manual review is requested, but the team is informed.\n\n### Example 3: Correlated Alarms\nWithin 3 minutes, three alarms fire: HighCPU, HighMemory, and HighNetworkIn all on the same EC2 instance group. Instead of three separate notifications, the agent groups them:\n\n> ðŸ”´ **INCIDENT**: Multiple alarms on prod-worker-group\n> â€¢ HighCPUUtilization: 92% (threshold: 85%)\n> â€¢ HighMemoryUtilization: 88% (threshold: 80%)\n> â€¢ HighNetworkIn: 1.2GB/min (threshold: 800MB/min)\n> **Assessment**: Likely traffic surge or resource contention. Recommend scaling review.",
      "errorHandling": "### AWS API Failures\n- If CloudWatch API returns a throttling error (HTTP 429), implement exponential backoff: wait 2s, 4s, 8s, then skip this polling cycle and log the failure. Resume normal polling on the next cycle.\n- If AWS credentials are expired or invalid (HTTP 403), post a warning to #infra-alerts in Slack: \"âš ï¸ AWS credentials expired â€” infrastructure monitoring paused. Please rotate IAM keys.\" Suspend polling until credentials are refreshed.\n- If a specific AWS region endpoint is unreachable, fall back to noting the outage and continue monitoring other regions.\n\n### Slack API Failures\n- If Slack returns `channel_not_found`, log the error and attempt to post to a fallback channel (#general). Store the failed message locally in `state/slack_queue.json` for retry.\n- If Slack rate-limits the agent (HTTP 429), respect the `Retry-After` header and queue messages for batch delivery.\n- If the bot token is revoked, write alerts to local file `state/pending_alerts.json` and note the credential issue in the next successful connection.\n\n### Jira API Failures\n- If Jira returns 404 for the project key, log the misconfiguration and include the ticket details in the Slack alert instead.\n- If duplicate detection search fails, err on the side of creating the ticket (better to have a duplicate than to miss an escalation).\n- If Jira is completely unreachable, store pending tickets in `state/jira_queue.json` and retry on the next polling cycle.\n\n### State File Corruption\n- If `alarm_tracker.json` or `cost_baselines.json` cannot be parsed, rename the corrupted file with a `.bak` suffix, reinitialize with empty state, and log a warning. The agent will rebuild baselines over the next 7 days.\n\n### General Resilience\n- Never let a failure in one integration (e.g., Jira down) block the entire monitoring loop. Each step should be independently resilient.\n- Maintain a local `state/agent_health.json` file tracking the last successful execution of each responsibility for self-diagnostics.",
      "customSections": [
        {
          "key": "alarm_severity_matrix",
          "label": "Alarm Severity Classification Matrix",
          "content": "## Severity Classification Rules\n\n**CRITICAL** (immediate Slack #incidents + Jira after 15 min):\n- Any alarm with 'prod' in the resource name and metric > 90% threshold\n- RDS connection count > 90% of max\n- ELB 5xx error rate > 5%\n- Any alarm in the 'AWS/Route53' or 'AWS/CloudFront' namespace\n\n**WARNING** (Slack #infra-alerts + Jira after 30 min):\n- Staging environment alarms exceeding thresholds\n- CPU/Memory between 80-90% on production resources\n- Disk utilization > 75%\n- Lambda error rate > 2%\n\n**INFO** (batched to #infra-monitoring every 10 min):\n- Development environment alarms\n- Metrics approaching thresholds but not yet breaching\n- INSUFFICIENT_DATA states lasting > 1 hour\n- Resolved alarm notifications"
        },
        {
          "key": "cost_thresholds",
          "label": "Cost Anomaly Thresholds",
          "content": "## Cost Monitoring Thresholds\n\n- **25-49% above rolling average**: Post informational alert to #finops. No manual review required.\n- **50-99% above rolling average**: Post urgent alert to #finops. Trigger manual_review for team confirmation before any automated actions.\n- **100%+ above rolling average**: Post critical alert to #finops AND #incidents. Create Jira ticket immediately. Trigger manual_review.\n- **Absolute threshold**: If total daily spend exceeds $5,000 (configurable), trigger alert regardless of percentage change.\n- **New service detection**: If a service appears in billing that has no baseline history, flag it as a new cost line item for team awareness."
        },
        {
          "key": "weekly_report_structure",
          "label": "Weekly Report Template",
          "content": "## Weekly Infrastructure Health Report Structure\n\n1. **Executive Summary**: 2-3 sentence overview of the week's infrastructure health. Overall status: Healthy / Degraded / Critical.\n2. **Service Health Scorecard**: Table of all monitored services with green/yellow/red status based on alarm frequency and severity.\n3. **Incident Timeline**: Chronological list of all CRITICAL and WARNING incidents with duration, impact, and resolution.\n4. **Alarm Statistics**: Total alarms triggered, mean time to resolution, most frequent alarm sources, alarm-to-ticket conversion rate.\n5. **Cost Analysis**: Week-over-week spend comparison by service, top 5 cost drivers, any anomalies detected.\n6. **Capacity Trends**: CPU, memory, network, and storage utilization trends with projected capacity timeline.\n7. **Recommendations**: Actionable items derived from the week's data â€” right-sizing suggestions, alarm tuning, cost optimization opportunities."
        }
      ]
    },
    "suggested_tools": [
      "http_request",
      "file_write",
      "file_read"
    ],
    "suggested_triggers": [
      {
        "trigger_type": "polling",
        "config": {
          "interval_seconds": 120
        },
        "description": "Polls CloudWatch alarms and metrics every 2 minutes to detect new ALARM states, track alarm persistence, and monitor infrastructure health in near-real-time."
      },
      {
        "trigger_type": "schedule",
        "config": {
          "cron": "0 9 * * 1"
        },
        "description": "Runs every Monday at 9:00 AM to compile and distribute the weekly infrastructure health report covering the previous 7 days of metrics, incidents, and cost data."
      }
    ],
    "full_prompt_markdown": "# Infrastructure Health Monitor\n\nYou are an Infrastructure Health Monitor â€” a vigilant DevOps intelligence agent that continuously watches AWS CloudWatch metrics and alarms, detects anomalies in infrastructure performance and cost, and ensures the operations team stays informed and responsive through Slack notifications and Jira ticket management.\n\n## Identity & Purpose\n\nYou replace four separate rigid automation workflows with a single reasoning-capable agent:\n1. CloudWatch alarm â†’ Slack notification workflow\n2. CloudWatch alarm â†’ Jira ticket creation workflow\n3. AWS cost â†’ anomaly alert workflow\n4. CloudWatch â†’ weekly health report workflow\n\nYour advantage over static automations: you can correlate multiple alarms into a single incident, classify severity based on context, suppress notification noise, track alarm persistence intelligently, and produce actionable weekly reports with recommendations.\n\n## Core Instructions\n\n### Polling Cycle (Every 2 Minutes)\n\n1. **Poll CloudWatch Alarms**: Query the AWS CloudWatch `DescribeAlarms` API filtered to `ALARM` and `INSUFFICIENT_DATA` states. Compare results against your tracked alarm state in `state/alarm_tracker.json`.\n\n2. **Classify New Alarms by Severity**:\n   - **CRITICAL**: Production alarms with metrics > 90% of threshold, Route53/CloudFront alarms, RDS connection saturation, ELB 5xx > 5%\n   - **WARNING**: Staging alarms, production metrics at 80-90%, disk > 75%, Lambda errors > 2%\n   - **INFO**: Dev environment alarms, approaching-threshold metrics, stale INSUFFICIENT_DATA\n\n3. **Correlate Related Alarms**: Group alarms firing within 5 minutes on the same resource/service into a single incident. Present a unified view rather than bombarding the team with individual alerts.\n\n4. **Post to Slack**:\n   - CRITICAL â†’ `#incidents` with `@channel` mention\n   - WARNING â†’ `#infra-alerts`\n   - INFO â†’ batch every 10 minutes to `#infra-monitoring`\n   - Use Slack Block Kit for rich formatting with severity indicators, metric values, and action links\n\n5. **Track Alarm Persistence**: Update `state/alarm_tracker.json` with alarm first-seen timestamps. Escalate to Jira when:\n   - CRITICAL alarms persist > 15 minutes\n   - WARNING alarms persist > 30 minutes\n\n6. **Create Jira Tickets**: Before creating, search for existing open tickets matching the alarm/resource. If found, add a comment. If not, create a new issue with:\n   - Severity label and priority mapping\n   - Affected resource details and alarm history\n   - Suggested remediation steps\n   - Link back to the Slack alert thread\n\n### Cost Anomaly Detection\n\n1. Query AWS Cost Explorer `GetCostAndUsage` API for current-day spend by service.\n2. Compare against 7-day rolling averages stored in `state/cost_baselines.json`.\n3. Alert thresholds:\n   - **25-49% above average**: Info alert to `#finops`\n   - **50-99% above average**: Urgent alert + `manual_review` request\n   - **100%+ above average**: Critical alert to `#finops` + `#incidents` + immediate Jira ticket\n   - **Absolute ceiling**: Alert if daily total exceeds $5,000 regardless of percentage\n4. Update rolling averages after each daily aggregation.\n\n### Weekly Health Report (Monday 9:00 AM)\n\n1. Gather 7-day CloudWatch metrics: CPU, memory, network, error rates, latency\n2. Compile alarm history: frequency, MTTR, top offenders\n3. Pull cost breakdown from Cost Explorer with week-over-week comparison\n4. Generate structured report with: Executive Summary, Service Health Scorecard, Incident Timeline, Alarm Statistics, Cost Analysis, Capacity Trends, Recommendations\n5. Post to `#infra-weekly` in Slack\n6. Create Jira epic for follow-up recommendations with individual stories\n\n## Tool Usage Guide\n\n### AWS (via http_request + aws connector)\n- `POST https://monitoring.{region}.amazonaws.com` â€” DescribeAlarms, DescribeAlarmHistory, GetMetricStatistics\n- `POST https://ce.us-east-1.amazonaws.com` â€” GetCostAndUsage (Cost Explorer)\n- Always include proper AWS Signature V4 headers\n\n### Slack (via http_request + slack connector)\n- `POST https://slack.com/api/chat.postMessage` â€” Send alerts with Block Kit formatting\n- `POST https://slack.com/api/chat.update` â€” Update existing alert messages\n- Use `thread_ts` for threaded replies on ongoing incidents\n- Authorization: `Bearer {bot_token}`\n\n### Jira (via http_request + jira connector)\n- `POST https://{domain}.atlassian.net/rest/api/3/issue` â€” Create issues and epics\n- `GET https://{domain}.atlassian.net/rest/api/3/search?jql=...` â€” Search for duplicates\n- `POST https://{domain}.atlassian.net/rest/api/3/issue/{key}/comment` â€” Add comments\n- Use Atlassian Document Format (ADF) for issue descriptions\n\n### Local Files (via file_read / file_write)\n- `state/alarm_tracker.json` â€” Alarm state, first-seen timestamps, escalation status\n- `state/cost_baselines.json` â€” 7-day rolling cost averages by service\n- `state/slack_queue.json` â€” Failed Slack messages for retry\n- `state/jira_queue.json` â€” Failed Jira tickets for retry\n- `state/agent_health.json` â€” Self-diagnostics and last-successful-run timestamps\n- `reports/weekly_{date}.md` â€” Weekly report drafts\n\n## Error Handling\n\n- **AWS throttling (429)**: Exponential backoff (2s, 4s, 8s), skip cycle if exhausted\n- **AWS auth failure (403)**: Alert #infra-alerts about expired credentials, pause polling\n- **Slack rate limit**: Respect `Retry-After`, queue messages locally\n- **Slack channel not found**: Fallback to #general, store in queue\n- **Jira unreachable**: Store in `state/jira_queue.json`, include details in Slack alert instead\n- **State file corruption**: Rename to `.bak`, reinitialize, log warning\n- **Principle**: Never let one integration failure block the entire monitoring loop\n\n## Communication Protocols\n\n- **user_message**: Slack channel posts for alerts and reports\n- **agent_memory**: Baseline metrics, alarm tracking state, cost rolling averages\n- **manual_review**: Cost spikes exceeding 50% of baseline require human confirmation",
    "summary": "The Infrastructure Health Monitor is a unified DevOps intelligence agent that replaces four separate CloudWatch automation workflows. It polls AWS CloudWatch every 2 minutes for alarm states, classifies severity, correlates related alarms into incidents, and routes notifications to appropriate Slack channels. Persistent alarms are automatically escalated to Jira ops tickets with duplicate detection. The agent tracks cost anomalies against 7-day rolling baselines and compiles comprehensive weekly health reports every Monday covering service health scores, incident timelines, cost analysis, and actionable recommendations.",
    "design_highlights": [
      {
        "category": "Intelligent Monitoring",
        "icon": "ðŸ”",
        "color": "blue",
        "items": [
          "Polls CloudWatch alarms every 2 minutes with state-diff tracking",
          "Three-tier severity classification (CRITICAL/WARNING/INFO) based on resource context",
          "Correlates multiple alarms on the same resource into unified incidents",
          "Tracks alarm persistence with automatic escalation timers"
        ]
      },
      {
        "category": "Cost Intelligence",
        "icon": "ðŸ’°",
        "color": "green",
        "items": [
          "Daily cost anomaly detection against 7-day rolling averages",
          "Tiered alerting: 25%, 50%, and 100% thresholds with escalating responses",
          "New cost line-item detection for unexpected service usage",
          "Week-over-week cost trend analysis in weekly reports"
        ]
      },
      {
        "category": "Smart Notifications",
        "icon": "ðŸ“¢",
        "color": "orange",
        "items": [
          "Severity-routed Slack channels: #incidents, #infra-alerts, #infra-monitoring",
          "Noise suppression via alarm correlation and batched INFO alerts",
          "Threaded Slack updates for ongoing incident tracking",
          "Rich Block Kit formatting with severity indicators and action links"
        ]
      },
      {
        "category": "Automated Ops Tickets",
        "icon": "ðŸŽ«",
        "color": "purple",
        "items": [
          "Automatic Jira ticket creation for persistent alarms",
          "Duplicate detection via JQL search before creating new tickets",
          "Comment-based updates on existing open tickets",
          "Weekly report generates Jira epics with recommendation stories"
        ]
      }
    ],
    "suggested_connectors": [
      {
        "name": "aws",
        "label": "AWS CloudWatch & Cost Explorer",
        "auth_type": "access_key",
        "credential_fields": [
          {
            "key": "access_key_id",
            "label": "AWS Access Key ID",
            "type": "text",
            "placeholder": "AKIAIOSFODNN7EXAMPLE",
            "helpText": "IAM Access Key ID from AWS Console â†’ IAM â†’ Users â†’ Security credentials",
            "required": true
          },
          {
            "key": "secret_access_key",
            "label": "AWS Secret Access Key",
            "type": "password",
            "placeholder": "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY",
            "helpText": "Secret key shown only once when creating the access key. If lost, create a new key pair.",
            "required": true
          },
          {
            "key": "region",
            "label": "Default AWS Region",
            "type": "text",
            "placeholder": "us-east-1",
            "helpText": "Primary region to monitor. Additional regions can be configured in the agent prompt.",
            "required": true
          }
        ],
        "setup_instructions": "1. Go to AWS Console â†’ IAM â†’ Users.\n2. Create a dedicated IAM user (e.g., 'personas-monitor') or use an existing service account.\n3. Attach these managed policies: CloudWatchReadOnlyAccess, AWSBillingReadOnlyAccess (or ce:GetCostAndUsage permission).\n4. Go to Security credentials tab â†’ Create access key â†’ Select 'Third-party service' use case.\n5. Copy the Access Key ID and Secret Access Key into the fields above.\n6. Set the region to your primary AWS region (e.g., us-east-1).",
        "related_tools": [
          "http_request"
        ],
        "related_triggers": [
          0,
          1
        ],
        "api_base_url": "https://monitoring.{region}.amazonaws.com",
        "role": "cloud_infra",
        "category": "devops"
      },
      {
        "name": "slack",
        "label": "Slack Workspace",
        "auth_type": "bot_token",
        "credential_fields": [
          {
            "key": "bot_token",
            "label": "Bot User OAuth Token",
            "type": "password",
            "placeholder": "xoxb-xxxxxxxxxxxx-xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxx",
            "helpText": "Found in Slack App settings â†’ OAuth & Permissions â†’ Bot User OAuth Token. Starts with 'xoxb-'.",
            "required": true
          }
        ],
        "setup_instructions": "1. Go to api.slack.com/apps and create a new Slack App (or use an existing one).\n2. Under OAuth & Permissions, add these Bot Token Scopes: chat:write, chat:write.public, channels:read.\n3. Install the app to your workspace.\n4. Copy the 'Bot User OAuth Token' (starts with xoxb-) into the field above.\n5. Invite the bot to these channels: #incidents, #infra-alerts, #infra-monitoring, #infra-weekly, #finops.\n6. To invite: open each channel â†’ type /invite @YourBotName.",
        "related_tools": [
          "http_request"
        ],
        "related_triggers": [
          0,
          1
        ],
        "api_base_url": "https://slack.com/api",
        "role": "chat_messaging",
        "category": "messaging"
      },
      {
        "name": "jira",
        "label": "Jira Cloud",
        "auth_type": "api_token",
        "credential_fields": [
          {
            "key": "email",
            "label": "Atlassian Account Email",
            "type": "text",
            "placeholder": "ops-bot@yourcompany.com",
            "helpText": "The email address associated with the Atlassian account that will create tickets.",
            "required": true
          },
          {
            "key": "api_token",
            "label": "Jira API Token",
            "type": "password",
            "placeholder": "ATATT3xFfGF0...",
            "helpText": "Generate at id.atlassian.com/manage-profile/security/api-tokens â†’ Create API token.",
            "required": true
          },
          {
            "key": "domain",
            "label": "Jira Domain",
            "type": "text",
            "placeholder": "yourcompany",
            "helpText": "Your Jira Cloud domain â€” the part before .atlassian.net in your Jira URL.",
            "required": true
          },
          {
            "key": "project_key",
            "label": "Ops Project Key",
            "type": "text",
            "placeholder": "OPS",
            "helpText": "The Jira project key where infrastructure ops tickets will be created (e.g., OPS, INFRA, DEVOPS).",
            "required": true
          }
        ],
        "setup_instructions": "1. Go to id.atlassian.com/manage-profile/security/api-tokens.\n2. Click 'Create API token', name it 'Infrastructure Health Monitor'.\n3. Copy the generated token into the API Token field above.\n4. Enter the email address of the Atlassian account (consider using a service account).\n5. Enter your Jira domain (e.g., if your Jira is at mycompany.atlassian.net, enter 'mycompany').\n6. Enter the project key for your ops/infrastructure project. Ensure the account has 'Create Issue' and 'Add Comments' permissions in that project.\n7. Verify the project has issue types: Task (for individual tickets) and Epic (for weekly report follow-ups).",
        "related_tools": [
          "http_request"
        ],
        "related_triggers": [
          0,
          1
        ],
        "api_base_url": "https://{domain}.atlassian.net/rest/api/3",
        "role": "project_tracking",
        "category": "development"
      }
    ],
    "suggested_notification_channels": [
      {
        "type": "slack",
        "description": "Primary notification channel for critical infrastructure alarms requiring immediate attention. Posts to #incidents with @channel mentions.",
        "required_connector": "slack",
        "config_hints": {
          "channel": "#incidents"
        }
      },
      {
        "type": "slack",
        "description": "Warning-level infrastructure alerts for degraded but non-critical conditions. Posted to #infra-alerts for team awareness.",
        "required_connector": "slack",
        "config_hints": {
          "channel": "#infra-alerts"
        }
      },
      {
        "type": "slack",
        "description": "Weekly infrastructure health reports delivered every Monday morning with metrics, incidents, costs, and recommendations.",
        "required_connector": "slack",
        "config_hints": {
          "channel": "#infra-weekly"
        }
      }
    ],
    "suggested_event_subscriptions": [
      {
        "event_type": "alarm_state_change",
        "description": "Internal event emitted when a CloudWatch alarm transitions between OK, ALARM, and INSUFFICIENT_DATA states. Triggers severity classification and notification routing."
      },
      {
        "event_type": "cost_anomaly_detected",
        "description": "Internal event emitted when daily spend for any AWS service exceeds the 7-day rolling average by more than 25%. Triggers cost alerting and potential manual review."
      },
      {
        "event_type": "alarm_escalation_due",
        "description": "Internal event emitted when a tracked alarm has persisted beyond its severity-based escalation threshold (15 min for CRITICAL, 30 min for WARNING). Triggers Jira ticket creation."
      },
      {
        "event_type": "weekly_report_due",
        "description": "Triggered by the Monday 9:00 AM schedule. Initiates the full weekly health report compilation workflow including metrics gathering, analysis, and distribution."
      }
    ],
    "use_case_flows": [
      {
        "id": "flow_alarm_monitoring",
        "name": "Real-Time Alarm Monitoring & Escalation",
        "description": "The core polling loop that detects new CloudWatch alarms, classifies severity, correlates related alarms, notifies via Slack, and escalates persistent issues to Jira.",
        "nodes": [
          {
            "id": "n1",
            "type": "start",
            "label": "Polling trigger fires",
            "detail": "Every 120 seconds the polling trigger initiates the monitoring cycle"
          },
          {
            "id": "n2",
            "type": "connector",
            "label": "Fetch CloudWatch alarms",
            "detail": "POST DescribeAlarms filtered to ALARM and INSUFFICIENT_DATA states",
            "connector": "aws"
          },
          {
            "id": "n3",
            "type": "action",
            "label": "Load alarm tracker state",
            "detail": "Read state/alarm_tracker.json to compare current vs. previous alarm states"
          },
          {
            "id": "n4",
            "type": "decision",
            "label": "New alarms detected?",
            "detail": "Compare fetched alarms against tracked state to identify newly triggered alarms"
          },
          {
            "id": "n5",
            "type": "action",
            "label": "Classify severity & correlate",
            "detail": "Assign CRITICAL/WARNING/INFO based on resource context. Group related alarms firing within 5 minutes on the same resource."
          },
          {
            "id": "n6",
            "type": "connector",
            "label": "Post alert to Slack",
            "detail": "POST chat.postMessage to severity-appropriate channel (#incidents, #infra-alerts, or batched to #infra-monitoring)",
            "connector": "slack"
          },
          {
            "id": "n7",
            "type": "action",
            "label": "Update alarm tracker",
            "detail": "Write updated alarm states, timestamps, and escalation timers to state/alarm_tracker.json"
          },
          {
            "id": "n8",
            "type": "decision",
            "label": "Alarm persisted past threshold?",
            "detail": "Check if CRITICAL alarm > 15 min or WARNING alarm > 30 min without resolution"
          },
          {
            "id": "n9",
            "type": "connector",
            "label": "Search existing Jira tickets",
            "detail": "GET /search with JQL to check for open tickets matching alarm name and resource",
            "connector": "jira"
          },
          {
            "id": "n10",
            "type": "decision",
            "label": "Duplicate ticket exists?",
            "detail": "Determine if an open Jira ticket already covers this alarm/resource combination"
          },
          {
            "id": "n11",
            "type": "connector",
            "label": "Create Jira ops ticket",
            "detail": "POST /issue with severity label, resource details, alarm history, and remediation suggestions",
            "connector": "jira"
          },
          {
            "id": "n12",
            "type": "connector",
            "label": "Add comment to existing ticket",
            "detail": "POST /issue/{key}/comment with latest alarm status update",
            "connector": "jira"
          },
          {
            "id": "n13",
            "type": "connector",
            "label": "Update Slack with ticket link",
            "detail": "POST chat.update to add Jira ticket link to the original Slack alert",
            "connector": "slack"
          },
          {
            "id": "n14",
            "type": "error",
            "label": "Handle API failure",
            "detail": "Queue failed operations locally for retry. Never let one integration failure block the entire loop."
          },
          {
            "id": "n15",
            "type": "end",
            "label": "Cycle complete",
            "detail": "Monitoring cycle ends. Next poll in 120 seconds."
          }
        ],
        "edges": [
          {
            "id": "e1",
            "source": "n1",
            "target": "n2"
          },
          {
            "id": "e2",
            "source": "n2",
            "target": "n3"
          },
          {
            "id": "e3",
            "source": "n3",
            "target": "n4"
          },
          {
            "id": "e4",
            "source": "n4",
            "target": "n5",
            "label": "Yes",
            "variant": "yes"
          },
          {
            "id": "e5",
            "source": "n4",
            "target": "n8",
            "label": "No â€” check persistence",
            "variant": "no"
          },
          {
            "id": "e6",
            "source": "n5",
            "target": "n6"
          },
          {
            "id": "e7",
            "source": "n6",
            "target": "n7"
          },
          {
            "id": "e8",
            "source": "n7",
            "target": "n8"
          },
          {
            "id": "e9",
            "source": "n8",
            "target": "n9",
            "label": "Yes",
            "variant": "yes"
          },
          {
            "id": "e10",
            "source": "n8",
            "target": "n15",
            "label": "No",
            "variant": "no"
          },
          {
            "id": "e11",
            "source": "n9",
            "target": "n10"
          },
          {
            "id": "e12",
            "source": "n10",
            "target": "n12",
            "label": "Yes",
            "variant": "yes"
          },
          {
            "id": "e13",
            "source": "n10",
            "target": "n11",
            "label": "No",
            "variant": "no"
          },
          {
            "id": "e14",
            "source": "n11",
            "target": "n13"
          },
          {
            "id": "e15",
            "source": "n12",
            "target": "n13"
          },
          {
            "id": "e16",
            "source": "n13",
            "target": "n15"
          },
          {
            "id": "e17",
            "source": "n2",
            "target": "n14",
            "label": "API Error",
            "variant": "error"
          },
          {
            "id": "e18",
            "source": "n14",
            "target": "n15",
            "variant": "error"
          }
        ]
      },
      {
        "id": "flow_cost_anomaly",
        "name": "Cost Anomaly Detection",
        "description": "Monitors daily AWS spend against rolling baselines, detects cost anomalies, and escalates through tiered alerting with manual review for significant spikes.",
        "nodes": [
          {
            "id": "c1",
            "type": "start",
            "label": "Cost check triggered",
            "detail": "Runs during each polling cycle, aggregated and evaluated daily"
          },
          {
            "id": "c2",
            "type": "connector",
            "label": "Query Cost Explorer",
            "detail": "POST GetCostAndUsage for current-day spend broken down by AWS service",
            "connector": "aws"
          },
          {
            "id": "c3",
            "type": "action",
            "label": "Load cost baselines",
            "detail": "Read state/cost_baselines.json containing 7-day rolling averages by service"
          },
          {
            "id": "c4",
            "type": "action",
            "label": "Calculate deviations",
            "detail": "Compare each service's current spend against its rolling average and compute percentage increase"
          },
          {
            "id": "c5",
            "type": "decision",
            "label": "Anomaly exceeds 25%?",
            "detail": "Check if any service spend deviates more than 25% from the 7-day rolling average"
          },
          {
            "id": "c6",
            "type": "decision",
            "label": "Spike severity level?",
            "detail": "Classify: 25-49% = Info, 50-99% = Urgent (needs manual review), 100%+ = Critical"
          },
          {
            "id": "c7",
            "type": "connector",
            "label": "Post to #finops",
            "detail": "POST chat.postMessage with cost anomaly details, affected service, current vs baseline spend",
            "connector": "slack"
          },
          {
            "id": "c8",
            "type": "event",
            "label": "Request manual review",
            "detail": "Emit manual_review event for cost spikes >= 50% requiring human confirmation"
          },
          {
            "id": "c9",
            "type": "connector",
            "label": "Create Jira cost ticket",
            "detail": "POST /issue for critical cost anomalies (100%+) with full spend analysis",
            "connector": "jira"
          },
          {
            "id": "c10",
            "type": "action",
            "label": "Update rolling baselines",
            "detail": "Write updated 7-day averages to state/cost_baselines.json with today's data"
          },
          {
            "id": "c11",
            "type": "error",
            "label": "Handle Cost Explorer failure",
            "detail": "Log the failure, skip cost check this cycle, use cached baseline data"
          },
          {
            "id": "c12",
            "type": "end",
            "label": "Cost check complete",
            "detail": "Daily cost analysis cycle concludes"
          }
        ],
        "edges": [
          {
            "id": "ce1",
            "source": "c1",
            "target": "c2"
          },
          {
            "id": "ce2",
            "source": "c2",
            "target": "c3"
          },
          {
            "id": "ce3",
            "source": "c3",
            "target": "c4"
          },
          {
            "id": "ce4",
            "source": "c4",
            "target": "c5"
          },
          {
            "id": "ce5",
            "source": "c5",
            "target": "c6",
            "label": "Yes",
            "variant": "yes"
          },
          {
            "id": "ce6",
            "source": "c5",
            "target": "c10",
            "label": "No anomaly",
            "variant": "no"
          },
          {
            "id": "ce7",
            "source": "c6",
            "target": "c7",
            "label": "25-49% Info"
          },
          {
            "id": "ce8",
            "source": "c6",
            "target": "c8",
            "label": "50%+ Urgent"
          },
          {
            "id": "ce9",
            "source": "c7",
            "target": "c10"
          },
          {
            "id": "ce10",
            "source": "c8",
            "target": "c9"
          },
          {
            "id": "ce11",
            "source": "c9",
            "target": "c10"
          },
          {
            "id": "ce12",
            "source": "c10",
            "target": "c12"
          },
          {
            "id": "ce13",
            "source": "c2",
            "target": "c11",
            "label": "API Error",
            "variant": "error"
          },
          {
            "id": "ce14",
            "source": "c11",
            "target": "c12",
            "variant": "error"
          }
        ]
      },
      {
        "id": "flow_weekly_report",
        "name": "Weekly Infrastructure Health Report",
        "description": "Compiles a comprehensive weekly health report every Monday by gathering metrics, alarm history, and cost data, then distributing via Slack and creating Jira follow-up items.",
        "nodes": [
          {
            "id": "w1",
            "type": "start",
            "label": "Monday schedule fires",
            "detail": "Cron trigger fires at 9:00 AM every Monday"
          },
          {
            "id": "w2",
            "type": "connector",
            "label": "Gather CloudWatch metrics",
            "detail": "POST GetMetricStatistics for 7-day CPU, memory, network, error rates, and latency across all services",
            "connector": "aws"
          },
          {
            "id": "w3",
            "type": "connector",
            "label": "Fetch alarm history",
            "detail": "POST DescribeAlarmHistory for the past 7 days to compile alarm frequency and resolution times",
            "connector": "aws"
          },
          {
            "id": "w4",
            "type": "connector",
            "label": "Pull cost breakdown",
            "detail": "POST GetCostAndUsage for weekly spend by service with week-over-week comparison",
            "connector": "aws"
          },
          {
            "id": "w5",
            "type": "action",
            "label": "Load incident tracking data",
            "detail": "Read state/alarm_tracker.json for the week's incident timeline and resolution data"
          },
          {
            "id": "w6",
            "type": "action",
            "label": "Compile health report",
            "detail": "Generate structured report: Executive Summary, Service Health Scores, Incident Timeline, Alarm Stats, Cost Analysis, Capacity Trends, Recommendations"
          },
          {
            "id": "w7",
            "type": "action",
            "label": "Save report draft",
            "detail": "Write report to reports/weekly_{date}.md for archival"
          },
          {
            "id": "w8",
            "type": "connector",
            "label": "Post report to #infra-weekly",
            "detail": "POST chat.postMessage with the full formatted report using Block Kit sections",
            "connector": "slack"
          },
          {
            "id": "w9",
            "type": "decision",
            "label": "Recommendations generated?",
            "detail": "Check if the analysis produced actionable recommendations for the team"
          },
          {
            "id": "w10",
            "type": "connector",
            "label": "Create Jira epic + stories",
            "detail": "POST /issue to create an Epic for the week's recommendations, then individual Stories for each action item",
            "connector": "jira"
          },
          {
            "id": "w11",
            "type": "error",
            "label": "Handle partial data failure",
            "detail": "If one data source fails, compile report with available data and note the gap. Never skip the report entirely."
          },
          {
            "id": "w12",
            "type": "end",
            "label": "Report delivered",
            "detail": "Weekly health report cycle complete"
          }
        ],
        "edges": [
          {
            "id": "we1",
            "source": "w1",
            "target": "w2"
          },
          {
            "id": "we2",
            "source": "w2",
            "target": "w3"
          },
          {
            "id": "we3",
            "source": "w3",
            "target": "w4"
          },
          {
            "id": "we4",
            "source": "w4",
            "target": "w5"
          },
          {
            "id": "we5",
            "source": "w5",
            "target": "w6"
          },
          {
            "id": "we6",
            "source": "w6",
            "target": "w7"
          },
          {
            "id": "we7",
            "source": "w7",
            "target": "w8"
          },
          {
            "id": "we8",
            "source": "w8",
            "target": "w9"
          },
          {
            "id": "we9",
            "source": "w9",
            "target": "w10",
            "label": "Yes",
            "variant": "yes"
          },
          {
            "id": "we10",
            "source": "w9",
            "target": "w12",
            "label": "No",
            "variant": "no"
          },
          {
            "id": "we11",
            "source": "w10",
            "target": "w12"
          },
          {
            "id": "we12",
            "source": "w2",
            "target": "w11",
            "label": "API Error",
            "variant": "error"
          },
          {
            "id": "we13",
            "source": "w3",
            "target": "w11",
            "label": "API Error",
            "variant": "error"
          },
          {
            "id": "we14",
            "source": "w11",
            "target": "w6",
            "variant": "error"
          }
        ]
      }
    ]
  }
}
