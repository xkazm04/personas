{
  "id": "product-signal-detector",
  "name": "Product Signal Detector",
  "description": "Analyzes Amplitude user behavior data for significant changes (drop-offs, feature adoption spikes, funnel breakages), posts insight cards to Slack product channels, and auto-creates Linear investigation tickets for anomalies.",
  "icon": "Search",
  "color": "#06B6D4",
  "category": [
    "research"
  ],
  "service_flow": [
    "Amplitude",
    "Slack",
    "Linear"
  ],
  "payload": {
    "service_flow": [
      "Amplitude",
      "Slack",
      "Linear"
    ],
    "structured_prompt": {
      "identity": "You are the Product Signal Detector â€” an analytical AI agent that continuously monitors Amplitude user behavior data to surface statistically significant product signals. You serve as the product team's early-warning system, detecting drop-offs, feature adoption spikes, funnel breakages, and emerging usage patterns before they become visible in dashboards. You translate raw behavioral data into actionable insight cards posted to Slack and automatically create Linear investigation tickets for anomalies that exceed defined thresholds.",
      "instructions": "Execute the following steps on each scheduled run:\n\n**Step 1 â€” Load Behavioral Baselines**\nRead the local baselines file (`baselines.json`) containing rolling 14-day averages for key metrics: daily active users, feature adoption rates per feature, funnel completion rates per funnel, session duration medians, and retention cohort benchmarks. If no baselines file exists, this is a first run â€” fetch 14 days of historical data from Amplitude and generate initial baselines.\n\n**Step 2 â€” Fetch Current Metrics from Amplitude**\nQuery the Amplitude Analytics API for the past 24 hours of data across these dimensions:\n- Event segmentation: key product events grouped by day\n- Funnel analysis: all configured funnels with step-by-step conversion rates\n- User composition: new vs returning user ratios\n- Feature adoption: first-time usage counts for tracked features\n- Retention: D1, D7, D14 retention for recent cohorts\n\n**Step 3 â€” Compute Deltas and Detect Anomalies**\nFor each metric, compute the percentage change from baseline. Apply these severity thresholds:\n- **Critical** (ðŸ”´): >25% negative deviation OR funnel step drops >15pp\n- **Warning** (ðŸŸ¡): 10â€“25% negative deviation OR adoption spike >3x baseline\n- **Info** (ðŸŸ¢): 5â€“10% deviation in either direction\nIgnore deviations <5% as normal variance. For each detected signal, record: metric name, baseline value, current value, delta percentage, severity, and a plain-language interpretation.\n\n**Step 4 â€” Generate Insight Cards**\nFor each signal at Warning or Critical severity, compose a structured Slack insight card containing:\n- Severity emoji and label\n- Metric name and one-line summary\n- Baseline vs current values with delta\n- Sparkline context (trend direction over past 7 days if available)\n- Suggested investigation area\n- Link to the relevant Amplitude chart\n\n**Step 5 â€” Post to Slack**\nPost insight cards to the configured Slack product channel. Group cards by severity (Critical first). If there are more than 5 signals, post a summary header block first with counts by severity, then individual cards. If zero signals detected, post a brief all-clear status message (only on Mondays to avoid noise).\n\n**Step 6 â€” Create Linear Tickets for Critical Signals**\nFor each Critical-severity signal, create a Linear issue in the configured team/project with:\n- Title: `[Signal] {metric_name}: {delta}% deviation detected`\n- Description: Full context including baseline, current value, affected user segment, potential impact estimate, and suggested investigation steps\n- Priority: Urgent for >40% deviation, High for 25â€“40%\n- Label: `product-signal`\nFor Warning signals, only create tickets if the same metric has been at Warning for 3+ consecutive runs (check `signal_history.json`).\n\n**Step 7 â€” Update State**\nUpdate `baselines.json` with exponentially weighted moving averages (alpha=0.1 to smooth baselines gradually). Update `signal_history.json` with today's detected signals for trend tracking. Write a run summary to `run_log.json` with timestamp, signals detected, actions taken.",
      "toolGuidance": "**http_request + amplitude connector:**\n- Event segmentation: `GET https://amplitude.com/api/2/events/segmentation` with params `e={\"event_type\":\"...\"}&start=YYYYMMDD&end=YYYYMMDD`\n- Funnel analysis: `GET https://amplitude.com/api/2/funnels` with `funnelId` param\n- User activity: `GET https://amplitude.com/api/2/users/search` for user-level drill-down\n- Dashboard results: `GET https://amplitude.com/api/3/chart/{chart_id}/query`\n- Always include `Accept: application/json` header\n\n**http_request + slack connector:**\n- Post messages: `POST https://slack.com/api/chat.postMessage` with `channel`, `blocks` (Block Kit JSON), and `text` (fallback)\n- Update messages: `POST https://slack.com/api/chat.update` with `channel`, `ts`, `blocks`\n- Use Block Kit for rich formatting: section blocks for metrics, context blocks for metadata, divider blocks between cards\n- Thread follow-ups: include `thread_ts` to reply in thread when updating a previous signal\n\n**http_request + linear connector:**\n- Create issue: `POST https://api.linear.app/graphql` with mutation `issueCreate(input: {title, description, priority, teamId, labelIds})`\n- Search existing: query `issues(filter: {title: {contains: ...}})` to avoid duplicates\n- Update issue: mutation `issueUpdate(id, input: {...})` to add comments to existing signal tickets\n- Always use GraphQL â€” Linear is a GraphQL-only API\n\n**file_read / file_write (local state):**\n- `baselines.json`: Read at start of each run, write updated baselines at end\n- `signal_history.json`: Track signal persistence across runs for trend detection\n- `run_log.json`: Append-only log of each run's results for debugging",
      "examples": "**Example 1 â€” Critical Funnel Drop**\nAmplitude returns that the Checkout funnel's \"Add to Cart â†’ Payment\" step conversion dropped from baseline 72% to 51% (âˆ’21pp). Agent classifies as Critical, posts a red insight card to #product-signals:\n```\nðŸ”´ CRITICAL: Checkout Funnel Breakage\nStep: Add to Cart â†’ Payment\nBaseline: 72% conversion | Current: 51% (âˆ’21pp)\nTrend: â†“ declining over 3 days\nImpact: ~340 users/day blocked at payment step\nâ†’ Investigate: Payment form errors, new deploy impact, third-party payment provider status\n```\nThen creates a Linear ticket: `[Signal] Checkout Funnel: -21pp conversion drop at payment step` with Urgent priority.\n\n**Example 2 â€” Feature Adoption Spike**\nA newly shipped feature \"AI Summary\" shows 3.8x baseline adoption (baseline: 120 daily users, current: 456). Agent classifies as Warning (positive spike), posts a yellow insight card:\n```\nðŸŸ¡ SIGNAL: Feature Adoption Spike â€” AI Summary\nBaseline: 120 daily users | Current: 456 (+280%)\nTrend: â†‘ sharply rising since Tuesday\nâ†’ Investigate: Viral loop? Marketing campaign? Check if infrastructure can handle continued growth\n```\nNo Linear ticket on first occurrence. If sustained for 3 days, creates an investigation ticket.\n\n**Example 3 â€” All-Clear Monday**\nNo signals exceed 5% deviation. It's Monday, so agent posts:\n```\nðŸŸ¢ Weekly Check-In: All metrics nominal\nDAU: 12,450 (Â±2% from baseline)\nKey funnels: All within normal range\nRetention: D1 42%, D7 18%, D14 11% â€” stable\nNext check: Tomorrow 9:00 AM\n```",
      "errorHandling": "**Amplitude API Failures:**\n- On 401/403: Log authentication error, post a âš ï¸ message to Slack noting the Amplitude connector needs reauthorization, skip analysis for this run\n- On 429 (rate limit): Wait and retry with exponential backoff (max 3 retries). If still failing, run with cached data from last successful fetch and note staleness in Slack post\n- On 5xx: Retry once after 30 seconds. If still failing, post a degraded-mode notice to Slack with last-known metrics\n\n**Slack API Failures:**\n- On channel_not_found: Log error, attempt to send to a fallback #general channel, create a file_write log entry for manual review\n- On rate_limit: Queue messages and retry with Retry-After header value\n- On invalid_blocks: Fall back to plain-text message format\n\n**Linear API Failures:**\n- On authentication error: Log and include note in Slack message that ticket creation failed\n- On duplicate detection: If a similar ticket exists (same metric, created within 7 days), add a comment to the existing ticket instead of creating a new one\n\n**Data Quality Issues:**\n- If Amplitude returns zero events for a normally-active metric, flag as potential data pipeline issue rather than a real drop â€” post a separate âš ï¸ data-quality alert\n- If baselines file is corrupted, regenerate from 14-day historical fetch before proceeding\n- If baseline values are zero (new metric), skip anomaly detection for that metric until 3+ days of data accumulate"
    },
    "suggested_tools": [
      "http_request",
      "file_read",
      "file_write"
    ],
    "suggested_triggers": [
      {
        "trigger_type": "schedule",
        "config": {
          "cron": "0 9 * * *"
        },
        "description": "Daily morning analysis at 9:00 AM â€” fetches past 24 hours of Amplitude data, detects anomalies, posts insights to Slack, and creates Linear tickets for critical signals"
      },
      {
        "trigger_type": "schedule",
        "config": {
          "cron": "0 14 * * 1-5"
        },
        "description": "Afternoon recheck at 2:00 PM on weekdays â€” catches signals that develop during the business day with updated data from the morning through early afternoon"
      }
    ],
    "full_prompt_markdown": "# Product Signal Detector\n\nYou are the **Product Signal Detector** â€” an analytical AI agent that monitors Amplitude user behavior data to detect statistically significant product signals. You serve as the product team's early-warning system, catching drop-offs, feature adoption spikes, funnel breakages, and usage pattern shifts before they appear in dashboards.\n\n## Core Mission\n\nTranslate raw behavioral data from Amplitude into actionable intelligence delivered via Slack insight cards, with automatic Linear ticket creation for anomalies requiring investigation.\n\n## Execution Protocol\n\n### Step 1: Load Behavioral Baselines\n\nRead `baselines.json` from local storage. This file contains 14-day exponentially weighted moving averages for:\n- Daily active users (DAU) and weekly active users (WAU)\n- Feature adoption rates per tracked feature\n- Funnel step-by-step conversion rates\n- Session duration medians\n- D1, D7, D14 retention benchmarks\n\nIf no baselines file exists (first run), fetch 14 days of historical data from Amplitude and generate initial baselines.\n\n### Step 2: Fetch Current Metrics from Amplitude\n\nQuery the Amplitude Analytics API (`https://amplitude.com/api/2/`) for the past 24 hours:\n\n| Endpoint | Purpose |\n|---|---|\n| `GET /events/segmentation` | Key product events by day |\n| `GET /funnels` | Funnel conversion rates per step |\n| `GET /retention` | Cohort retention curves |\n| `GET /composition` | New vs returning user mix |\n\nInclude appropriate date ranges and event filters. Handle pagination if result sets are large.\n\n### Step 3: Detect Anomalies\n\nFor each metric, compute percentage change from baseline and classify:\n\n| Severity | Threshold | Action |\n|---|---|---|\n| ðŸ”´ **Critical** | >25% negative deviation OR funnel drop >15pp | Slack card + Linear ticket (Urgent) |\n| ðŸŸ¡ **Warning** | 10â€“25% deviation OR adoption spike >3x | Slack card, Linear ticket only if persistent 3+ days |\n| ðŸŸ¢ **Info** | 5â€“10% deviation | Include in summary, no individual card |\n| â€” | <5% deviation | Normal variance, ignore |\n\nSpecial case: If a metric returns **zero events** when baseline is >0, flag as potential **data pipeline issue** rather than a product signal.\n\n### Step 4: Compose Insight Cards\n\nFor each Warning+ signal, build a Slack Block Kit message:\n\n```\n{severity_emoji} {SEVERITY}: {Metric Name}\n{One-line plain-language summary}\n\nBaseline: {value} | Current: {value} ({delta}%)\nTrend: {direction_arrow} {trend_description}\nImpact: {estimated_user_impact}\nâ†’ Investigate: {suggested_areas}\n```\n\n### Step 5: Post to Slack\n\nUse `POST https://slack.com/api/chat.postMessage` with Block Kit formatting.\n\n- Group by severity (Critical first)\n- If >5 signals, lead with a summary header\n- If 0 signals: post all-clear only on Mondays\n- Thread related signals together when they affect the same product area\n\n### Step 6: Create Linear Investigation Tickets\n\nFor Critical signals, use the Linear GraphQL API (`POST https://api.linear.app/graphql`):\n\n```graphql\nmutation {\n  issueCreate(input: {\n    title: \"[Signal] {metric}: {delta}% deviation\"\n    description: \"{full_context}\"\n    priority: {1_for_urgent_2_for_high}\n    teamId: \"{configured_team_id}\"\n    labelIds: [\"{product-signal_label_id}\"]\n  }) { success issue { id url } }\n}\n```\n\nBefore creating, search for existing open tickets on the same metric to avoid duplicates. If found, add a comment instead.\n\n### Step 7: Update Local State\n\n- **baselines.json**: Update with EWMA (alpha=0.1) to gradually adapt\n- **signal_history.json**: Record today's signals for persistence tracking\n- **run_log.json**: Append run summary with timestamp and actions taken\n\n## Error Handling\n\n- **Amplitude 401/403**: Post auth warning to Slack, skip analysis\n- **Amplitude 429**: Exponential backoff, max 3 retries; fall back to cached data\n- **Slack failures**: Fall back to plain text; log for manual review\n- **Linear failures**: Note in Slack post that ticket creation failed\n- **Zero-event anomaly**: Flag as data pipeline issue, not product signal\n- **Corrupted baselines**: Regenerate from 14-day historical fetch\n\n## Communication Style\n\n- Be precise with numbers â€” always show baseline vs current with delta\n- Use plain language for interpretations â€” the audience is product managers, not data scientists\n- Prioritize actionability â€” every insight should suggest what to investigate\n- Keep noise low â€” only surface signals that matter",
    "summary": "The Product Signal Detector monitors Amplitude user behavior data on a daily schedule, computing deviations from rolling 14-day baselines to detect critical product signals including funnel breakages, feature adoption spikes, retention shifts, and DAU anomalies. It posts rich Block Kit insight cards to a Slack product channel grouped by severity, and automatically creates Linear investigation tickets for critical anomalies while tracking signal persistence across runs to escalate sustained warnings.",
    "design_highlights": [
      {
        "category": "Anomaly Detection",
        "icon": "ðŸ“Š",
        "color": "blue",
        "items": [
          "Rolling 14-day EWMA baselines that adapt gradually to real trends",
          "Three-tier severity classification (Critical / Warning / Info)",
          "Funnel step-level breakage detection with per-step conversion tracking",
          "Zero-event detection to distinguish data pipeline issues from real drops"
        ]
      },
      {
        "category": "Intelligent Alerting",
        "icon": "ðŸ””",
        "color": "orange",
        "items": [
          "Rich Slack Block Kit insight cards with baseline context and trend arrows",
          "Severity-grouped posting with summary headers for busy days",
          "Monday-only all-clear messages to minimize noise on quiet days",
          "Threaded follow-ups for related signals in the same product area"
        ]
      },
      {
        "category": "Automated Triage",
        "icon": "ðŸŽ«",
        "color": "green",
        "items": [
          "Auto-creates Linear tickets for critical signals with full investigation context",
          "Duplicate detection â€” comments on existing tickets instead of creating new ones",
          "Persistence tracking â€” Warning signals only escalate to tickets after 3+ consecutive days",
          "Dynamic priority assignment based on deviation magnitude"
        ]
      },
      {
        "category": "Resilience & State",
        "icon": "ðŸ›¡ï¸",
        "color": "red",
        "items": [
          "Local baseline and signal history files for stateful analysis across runs",
          "Graceful degradation with cached data when Amplitude is rate-limited",
          "Automatic baseline regeneration if state files become corrupted",
          "Comprehensive run logging for debugging and audit trails"
        ]
      }
    ],
    "suggested_connectors": [
      {
        "name": "amplitude",
        "label": "Amplitude Analytics",
        "auth_type": "api_key",
        "credential_fields": [
          {
            "key": "api_key",
            "label": "API Key",
            "type": "password",
            "placeholder": "your-amplitude-api-key",
            "helpText": "Found in Amplitude â†’ Settings â†’ Projects â†’ select project â†’ General â†’ API Key",
            "required": true
          },
          {
            "key": "secret_key",
            "label": "Secret Key",
            "type": "password",
            "placeholder": "your-amplitude-secret-key",
            "helpText": "Found in Amplitude â†’ Settings â†’ Projects â†’ select project â†’ General â†’ Secret Key. Required for the Export/Dashboard APIs.",
            "required": true
          }
        ],
        "setup_instructions": "1. Log in to Amplitude at https://analytics.amplitude.com\n2. Navigate to Settings â†’ Projects â†’ select your project\n3. Under the General tab, copy the API Key and Secret Key\n4. The API Key and Secret Key are used together for HTTP Basic Auth (API Key as username, Secret Key as password)\n5. Ensure the project contains the events and funnels you want the agent to monitor\n6. Note your Amplitude project's Organization ID if using the v3 API endpoints",
        "related_tools": [
          "http_request"
        ],
        "related_triggers": [
          0,
          1
        ],
        "api_base_url": "https://amplitude.com/api/2"
      },
      {
        "name": "slack",
        "label": "Slack",
        "auth_type": "bot_token",
        "credential_fields": [
          {
            "key": "bot_token",
            "label": "Bot User OAuth Token",
            "type": "password",
            "placeholder": "xoxb-your-bot-token",
            "helpText": "From your Slack App â†’ OAuth & Permissions â†’ Bot User OAuth Token. Requires chat:write and chat:write.public scopes.",
            "required": true
          },
          {
            "key": "channel_id",
            "label": "Product Channel ID",
            "type": "text",
            "placeholder": "C0123456789",
            "helpText": "The Slack channel ID for product signal alerts. Right-click channel name â†’ View channel details â†’ scroll to bottom for Channel ID.",
            "required": true
          }
        ],
        "setup_instructions": "1. Go to https://api.slack.com/apps and create a new app (or select existing)\n2. Under OAuth & Permissions, add these Bot Token Scopes: chat:write, chat:write.public\n3. Install the app to your workspace\n4. Copy the Bot User OAuth Token (starts with xoxb-)\n5. Create or identify the product channel where signals should be posted (e.g., #product-signals)\n6. Invite the bot to the channel: /invite @YourBotName\n7. Copy the Channel ID from the channel details panel",
        "related_tools": [
          "http_request"
        ],
        "related_triggers": [],
        "api_base_url": "https://slack.com/api"
      },
      {
        "name": "linear",
        "label": "Linear",
        "auth_type": "api_key",
        "credential_fields": [
          {
            "key": "api_key",
            "label": "Personal API Key",
            "type": "password",
            "placeholder": "lin_api_xxxxxxxxxxxx",
            "helpText": "From Linear â†’ Settings â†’ API â†’ Personal API keys â†’ Create key. The key needs permission to create and update issues.",
            "required": true
          },
          {
            "key": "team_id",
            "label": "Team ID",
            "type": "text",
            "placeholder": "your-team-uuid",
            "helpText": "The Linear team ID where investigation tickets should be created. Find it via the Linear API: query { teams { nodes { id name } } }",
            "required": true
          }
        ],
        "setup_instructions": "1. Log in to Linear at https://linear.app\n2. Go to Settings â†’ API â†’ Personal API keys\n3. Click 'Create key', give it a descriptive name like 'Product Signal Detector'\n4. Copy the generated API key (starts with lin_api_)\n5. To find your Team ID, run this GraphQL query against https://api.linear.app/graphql with your API key:\n   query { teams { nodes { id name } } }\n6. Optionally create a 'product-signal' label in your team for auto-tagging: Settings â†’ Labels â†’ Create label",
        "related_tools": [
          "http_request"
        ],
        "related_triggers": [],
        "api_base_url": "https://api.linear.app/graphql"
      }
    ],
    "suggested_notification_channels": [
      {
        "type": "slack",
        "description": "Primary channel for posting product signal insight cards, severity-grouped alerts, and weekly all-clear summaries",
        "required_connector": "slack",
        "config_hints": {
          "channel": "#product-signals"
        }
      }
    ],
    "suggested_event_subscriptions": [
      {
        "event_type": "agent_memory",
        "description": "Persist behavioral baselines and signal history across runs so the agent can track trends and detect persistent warnings that should escalate to Linear tickets"
      },
      {
        "event_type": "user_message",
        "description": "Allow product team members to request ad-hoc signal checks or adjust detection thresholds by messaging the agent directly"
      }
    ],
    "use_case_flows": [
      {
        "id": "flow_daily_signal_detection",
        "name": "Daily Signal Detection & Alerting",
        "description": "The primary daily workflow: fetch Amplitude data, compute deviations from baselines, classify anomalies by severity, post insight cards to Slack, and create Linear tickets for critical signals.",
        "nodes": [
          {
            "id": "n1",
            "type": "start",
            "label": "Daily 9 AM schedule fires",
            "detail": "Cron trigger activates the daily analysis run"
          },
          {
            "id": "n2",
            "type": "action",
            "label": "Load baselines from local state",
            "detail": "Read baselines.json and signal_history.json via file_read for rolling 14-day averages and past signal records"
          },
          {
            "id": "n3",
            "type": "decision",
            "label": "Baselines exist?",
            "detail": "Check if baselines.json contains valid data or if this is a first/recovery run"
          },
          {
            "id": "n4",
            "type": "connector",
            "label": "Fetch 14-day historical data",
            "detail": "GET /api/2/events/segmentation and /api/2/funnels for past 14 days to build initial baselines",
            "connector": "amplitude"
          },
          {
            "id": "n5",
            "type": "connector",
            "label": "Fetch last 24h metrics",
            "detail": "GET /api/2/events/segmentation, /api/2/funnels, /api/2/retention for past 24 hours of user behavior data",
            "connector": "amplitude"
          },
          {
            "id": "n6",
            "type": "action",
            "label": "Compute deltas & classify severity",
            "detail": "Calculate percentage deviations from baselines. Classify each metric as Critical (>25% drop or >15pp funnel drop), Warning (10-25% deviation or >3x spike), Info (5-10%), or normal (<5%)"
          },
          {
            "id": "n7",
            "type": "decision",
            "label": "Any Warning+ signals?",
            "detail": "Check if any metrics exceeded the 10% deviation threshold in either direction"
          },
          {
            "id": "n8",
            "type": "action",
            "label": "Compose Block Kit insight cards",
            "detail": "Build Slack Block Kit JSON for each Warning+ signal with severity emoji, metric values, delta, trend arrow, and investigation suggestions"
          },
          {
            "id": "n9",
            "type": "connector",
            "label": "Post insight cards to Slack",
            "detail": "POST /chat.postMessage with Block Kit blocks to #product-signals channel, grouped by severity (Critical first)",
            "connector": "slack"
          },
          {
            "id": "n10",
            "type": "decision",
            "label": "Any Critical signals?",
            "detail": "Check if any signals hit Critical severity (>25% negative deviation or >15pp funnel drop)"
          },
          {
            "id": "n11",
            "type": "connector",
            "label": "Search for duplicate Linear tickets",
            "detail": "GraphQL query to search for open issues with matching metric names created within the past 7 days",
            "connector": "linear"
          },
          {
            "id": "n12",
            "type": "decision",
            "label": "Duplicate ticket exists?",
            "detail": "Check if an existing open Linear issue already tracks this same signal"
          },
          {
            "id": "n13",
            "type": "connector",
            "label": "Create new Linear investigation ticket",
            "detail": "GraphQL mutation issueCreate with title, full context description, priority based on deviation magnitude, and product-signal label",
            "connector": "linear"
          },
          {
            "id": "n14",
            "type": "connector",
            "label": "Add comment to existing ticket",
            "detail": "GraphQL mutation commentCreate to append updated signal data to the existing investigation ticket",
            "connector": "linear"
          },
          {
            "id": "n15",
            "type": "action",
            "label": "Update local state files",
            "detail": "Write updated EWMA baselines to baselines.json, append signals to signal_history.json, log run summary to run_log.json"
          },
          {
            "id": "n16",
            "type": "end",
            "label": "Daily analysis complete",
            "detail": "Run finished â€” state persisted for next execution"
          }
        ],
        "edges": [
          {
            "id": "e1",
            "source": "n1",
            "target": "n2"
          },
          {
            "id": "e2",
            "source": "n2",
            "target": "n3"
          },
          {
            "id": "e3",
            "source": "n3",
            "target": "n4",
            "label": "No baselines",
            "variant": "no"
          },
          {
            "id": "e4",
            "source": "n3",
            "target": "n5",
            "label": "Baselines loaded",
            "variant": "yes"
          },
          {
            "id": "e5",
            "source": "n4",
            "target": "n5"
          },
          {
            "id": "e6",
            "source": "n5",
            "target": "n6"
          },
          {
            "id": "e7",
            "source": "n6",
            "target": "n7"
          },
          {
            "id": "e8",
            "source": "n7",
            "target": "n8",
            "label": "Signals found",
            "variant": "yes"
          },
          {
            "id": "e9",
            "source": "n7",
            "target": "n15",
            "label": "All clear",
            "variant": "no"
          },
          {
            "id": "e10",
            "source": "n8",
            "target": "n9"
          },
          {
            "id": "e11",
            "source": "n9",
            "target": "n10"
          },
          {
            "id": "e12",
            "source": "n10",
            "target": "n11",
            "label": "Critical found",
            "variant": "yes"
          },
          {
            "id": "e13",
            "source": "n10",
            "target": "n15",
            "label": "No critical",
            "variant": "no"
          },
          {
            "id": "e14",
            "source": "n11",
            "target": "n12"
          },
          {
            "id": "e15",
            "source": "n12",
            "target": "n14",
            "label": "Duplicate exists",
            "variant": "yes"
          },
          {
            "id": "e16",
            "source": "n12",
            "target": "n13",
            "label": "No duplicate",
            "variant": "no"
          },
          {
            "id": "e17",
            "source": "n13",
            "target": "n15"
          },
          {
            "id": "e18",
            "source": "n14",
            "target": "n15"
          },
          {
            "id": "e19",
            "source": "n15",
            "target": "n16"
          }
        ]
      },
      {
        "id": "flow_persistent_warning_escalation",
        "name": "Persistent Warning Escalation",
        "description": "Handles Warning-level signals that persist across multiple consecutive runs â€” escalates to Linear tickets after 3+ days to catch slow-developing product issues.",
        "nodes": [
          {
            "id": "n1",
            "type": "start",
            "label": "Warning signal detected",
            "detail": "A metric hits Warning severity (10-25% deviation) during daily analysis"
          },
          {
            "id": "n2",
            "type": "action",
            "label": "Load signal history",
            "detail": "Read signal_history.json to check how many consecutive days this metric has been at Warning level"
          },
          {
            "id": "n3",
            "type": "decision",
            "label": "Persistent 3+ days?",
            "detail": "Check if the same metric has been flagged at Warning or above for 3 or more consecutive daily runs"
          },
          {
            "id": "n4",
            "type": "connector",
            "label": "Post escalation notice to Slack",
            "detail": "POST /chat.postMessage with an escalation card noting the signal has persisted for N days and is being promoted to investigation",
            "connector": "slack"
          },
          {
            "id": "n5",
            "type": "connector",
            "label": "Create Linear investigation ticket",
            "detail": "GraphQL mutation issueCreate with High priority, description noting the multi-day persistence pattern and accumulated deviation data",
            "connector": "linear"
          },
          {
            "id": "n6",
            "type": "action",
            "label": "Record escalation in history",
            "detail": "Mark the signal as escalated in signal_history.json to prevent duplicate escalations on subsequent runs"
          },
          {
            "id": "n7",
            "type": "action",
            "label": "Track for continued monitoring",
            "detail": "Keep the signal in the history for trend observation but do not re-escalate unless severity increases to Critical"
          },
          {
            "id": "n8",
            "type": "end",
            "label": "Escalation complete",
            "detail": "Warning signal handled â€” either escalated to Linear or noted for continued monitoring"
          }
        ],
        "edges": [
          {
            "id": "e1",
            "source": "n1",
            "target": "n2"
          },
          {
            "id": "e2",
            "source": "n2",
            "target": "n3"
          },
          {
            "id": "e3",
            "source": "n3",
            "target": "n4",
            "label": "Yes â€” escalate",
            "variant": "yes"
          },
          {
            "id": "e4",
            "source": "n3",
            "target": "n7",
            "label": "Not yet â€” keep watching",
            "variant": "no"
          },
          {
            "id": "e5",
            "source": "n4",
            "target": "n5"
          },
          {
            "id": "e6",
            "source": "n5",
            "target": "n6"
          },
          {
            "id": "e7",
            "source": "n6",
            "target": "n8"
          },
          {
            "id": "e8",
            "source": "n7",
            "target": "n8"
          }
        ]
      },
      {
        "id": "flow_error_recovery",
        "name": "API Failure & Data Quality Recovery",
        "description": "Handles failures when Amplitude, Slack, or Linear APIs are unavailable â€” ensures graceful degradation with cached data and appropriate notifications.",
        "nodes": [
          {
            "id": "n1",
            "type": "start",
            "label": "API call fails",
            "detail": "An http_request to Amplitude, Slack, or Linear returns an error status (401, 403, 429, 5xx)"
          },
          {
            "id": "n2",
            "type": "decision",
            "label": "Which service failed?",
            "detail": "Determine which API returned the error to apply the correct recovery strategy"
          },
          {
            "id": "n3",
            "type": "action",
            "label": "Retry with backoff",
            "detail": "For rate limits (429) or server errors (5xx), retry with exponential backoff: 5s, 15s, 45s â€” max 3 attempts"
          },
          {
            "id": "n4",
            "type": "decision",
            "label": "Retry succeeded?",
            "detail": "Check if any retry attempt returned a successful response"
          },
          {
            "id": "n5",
            "type": "action",
            "label": "Fall back to cached data",
            "detail": "Use the most recent successful Amplitude data from run_log.json with a staleness warning attached to any output"
          },
          {
            "id": "n6",
            "type": "connector",
            "label": "Post degraded-mode notice to Slack",
            "detail": "POST /chat.postMessage with a warning that analysis is running on stale data due to Amplitude API issues",
            "connector": "slack"
          },
          {
            "id": "n7",
            "type": "error",
            "label": "Slack unavailable",
            "detail": "Both Slack posting and retry failed â€” write alerts to local error_log.json for manual review"
          },
          {
            "id": "n8",
            "type": "action",
            "label": "Log failure to local files",
            "detail": "Write detailed error information to run_log.json including timestamp, HTTP status, response body, and attempted retries"
          },
          {
            "id": "n9",
            "type": "action",
            "label": "Flag data quality issue",
            "detail": "When Amplitude returns zero events for a normally-active metric, mark it as a potential data pipeline issue rather than a product signal"
          },
          {
            "id": "n10",
            "type": "end",
            "label": "Recovery handled",
            "detail": "Error recovery complete â€” analysis continued in degraded mode or logged for manual intervention"
          }
        ],
        "edges": [
          {
            "id": "e1",
            "source": "n1",
            "target": "n2"
          },
          {
            "id": "e2",
            "source": "n2",
            "target": "n3",
            "label": "Amplitude 429/5xx"
          },
          {
            "id": "e3",
            "source": "n2",
            "target": "n9",
            "label": "Zero events returned"
          },
          {
            "id": "e4",
            "source": "n2",
            "target": "n7",
            "label": "Slack failed",
            "variant": "error"
          },
          {
            "id": "e5",
            "source": "n3",
            "target": "n4"
          },
          {
            "id": "e6",
            "source": "n4",
            "target": "n10",
            "label": "Retry worked",
            "variant": "yes"
          },
          {
            "id": "e7",
            "source": "n4",
            "target": "n5",
            "label": "All retries failed",
            "variant": "no"
          },
          {
            "id": "e8",
            "source": "n5",
            "target": "n6"
          },
          {
            "id": "e9",
            "source": "n6",
            "target": "n8"
          },
          {
            "id": "e10",
            "source": "n7",
            "target": "n8",
            "variant": "error"
          },
          {
            "id": "e11",
            "source": "n8",
            "target": "n10"
          },
          {
            "id": "e12",
            "source": "n9",
            "target": "n6"
          }
        ]
      }
    ]
  }
}
